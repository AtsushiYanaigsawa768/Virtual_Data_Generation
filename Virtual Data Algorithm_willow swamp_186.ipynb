{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qingxinxia/OpenPackChallenge2025/blob/main/1.Data%20Augmentation%20Algorithm%20with%20HAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f251e07c11db0713",
      "metadata": {
        "collapsed": false,
        "id": "f251e07c11db0713"
      },
      "source": [
        "# Virtual Data Generation for Complex Industrial Activity Recognition\n",
        "\n",
        "\n",
        "\n",
        "This notebook has been designed for the 7th Factory Work Activity Recognition Challenge competition with the aim of Activity Recognition using REAL Accelerometer from OpenPack dataset and GENERATED Accelerometer created by participants.\n",
        "\n",
        "If you have any questions, please feel free to email qingxinxia@hkust-gz.edu.cn with the subject Factory Work Activity Recognition Challenge.\n",
        "\n",
        "About this dataset and challenge -> https://abc-research.github.io/challenge2025/\n",
        "\n",
        "This notebook was prepared by Qingxin Xia, Kei Tanigaki and Yoshimura Naoya.\n",
        "\n",
        "---\n",
        "\n",
        "# 工場作業行動認識のための仮想データ生成\n",
        "本ノートブックは、第7回工場作業行動認識チャレンジのために設計されました。本チャレンジでは、OpenPackデータセット作成に用いられた実際の加速度計と参加者によって生成された加速度センサデータを使用しています。\n",
        "\n",
        "ご質問がある場合は、件名を「工場作業行動認識チャレンジ」として、qingxinxia@hkust-gz.edu.cnまでお気軽にメールしてください。\n",
        "\n",
        "このデータセットとチャレンジについて -> https://abc-research.github.io/challenge2025/\n",
        "\n",
        "このノートブックはQingxin Xia, Kei TanigakiとYoshimura Naoyaによって準備されました。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kDKmsfvHW7SC",
      "metadata": {
        "id": "kDKmsfvHW7SC"
      },
      "source": [
        "# 1. Preparation\n",
        "\n",
        "---\n",
        "\n",
        "# 1. 準備する"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d843c988b9ef7b0",
      "metadata": {
        "collapsed": false,
        "id": "9d843c988b9ef7b0"
      },
      "source": [
        "## 1.1 Mount Drive\n",
        "\n",
        "This tutorial is made in Google Colab. So, first we need to connect the Google Drive to access the data. You can directly add folder path to access the data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Mount Drive\n",
        "\n",
        "本チュートリアルは Google Colab で作成されています。\n",
        "そのため、まず Google ドライブに接続してデータにアクセスする必要があります。\n",
        "フォルダーパスを追加することでドライブへのアクセスが可能になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943f2d92923662a1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-25T05:26:29.294878400Z",
          "start_time": "2024-11-25T05:26:29.291889Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "943f2d92923662a1",
        "outputId": "70510a44-d012-4c4f-a9c7-dae0c4c51d9c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adde54465537d1b3",
      "metadata": {
        "collapsed": false,
        "id": "adde54465537d1b3"
      },
      "source": [
        "## 1.2 Load Necessary Libraries and Prepare Environment\n",
        "\n",
        "---\n",
        "## 1.2 必要なライブラリをロードして環境を準備する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2574b92f4441c5d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:40:37.951189Z",
          "start_time": "2024-11-26T17:40:35.164127100Z"
        },
        "id": "a2574b92f4441c5d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from scipy.interpolate import CubicSpline  # for warping\n",
        "from einops import rearrange, repeat\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e4e4342fba656c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:35.657476100Z",
          "start_time": "2024-11-26T17:32:35.555296600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e4e4342fba656c",
        "outputId": "c42b4574-f614-4f64-dd01-d88b905c7632"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIUkKzP8Lxak",
      "metadata": {
        "id": "oIUkKzP8Lxak"
      },
      "source": [
        "## 1.3 Fixed Part\n",
        "\n",
        "Participants can change the splits portion and random seed to measure the robustness of their algorithms. When we evaluate the code, selected_columns and new_columns will not change. However, the split and random seed will change.\n",
        "\n",
        "\n",
        "---\n",
        "## 1.3 修正不可能な箇所\n",
        "\n",
        "参加者は、splitとrandom seedを変更して、アルゴリズムの堅牢性を測定できます。我々がコードを評価する際には、selected_columns と new_columns は変更されませんが、split と random seed は変更されます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2597443fadcb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2597443fadcb88",
        "outputId": "4f4ee93f-4aa0-43da-a219-ca8de75f65f9"
      },
      "outputs": [],
      "source": [
        "splits = [0.7, 0.1, 0.2]\n",
        "print('Randomly Split the real dataset into train, validation and test sets: %s'%str(splits))\n",
        "\n",
        "selected_columns = ['atr01/acc_x','atr01/acc_y','atr01/acc_z','atr02/acc_x','atr02/acc_y','atr02/acc_z','timestamp','operation']\n",
        "print('Select acceleration data of both wrists: %s'%selected_columns)\n",
        "\n",
        "new_columns = selected_columns[:6] + [selected_columns[-1]]\n",
        "print('Data for train, validation, and test: %s'%new_columns)\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    # Set seed for Python's random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Set seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # If using CUDA, set seed for GPU as well\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "\n",
        "# Set a fixed random seed\n",
        "seed_value = 2025\n",
        "set_random_seed(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2caace48e5668f",
      "metadata": {
        "collapsed": false,
        "id": "af2caace48e5668f"
      },
      "source": [
        "## 1.4 Create Folders\n",
        "\n",
        "Firstly, a folder is created to save OpenPack dataset: '/data/real/'.\n",
        "\n",
        "Participants can download OpenPack dataset by themselves. The data should be placed at: '/data/real/'.\n",
        "\n",
        "Another folder '/data/virtual/' will be created to save generated data.\n",
        " *(Note that, the size of this 'virtual' folder is limited to 500MB.)*\n",
        "\n",
        "---\n",
        "## 1.4 フォルダーの作成\n",
        "\n",
        "まず、OpenPack データセットを保存するためのフォルダー「/data/real/」が作成されます。\n",
        "\n",
        "参加者は OpenPack データセットを自分でダウンロードできます。データは「/data/real/」に配置する必要があります。\n",
        "\n",
        "生成されたデータを保存するために、別のフォルダー「/data/virtual/」が作成されます。\n",
        "\n",
        "*(この「virtual」フォルダーのサイズは500MBに制限されていることに注意してください。)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T15:23:45.250297800Z",
          "start_time": "2024-11-26T15:23:45.119289600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "08c306b8-be5b-45ba-debb-b68ae676f1fc"
      },
      "outputs": [],
      "source": [
        "realpath = r'/data/real'\n",
        "virtpath = r'/data/virtual'\n",
        "rootdir = r'/Virtual_Data_Generation/'  # replace with your project path\n",
        "real_directory = rootdir + realpath\n",
        "virt_directory = rootdir + virtpath\n",
        "\n",
        "# Create the directory\n",
        "os.makedirs(real_directory, exist_ok=True)\n",
        "print(f\"Directory '{realpath}' created successfully.\")\n",
        "\n",
        "# Create the directory\n",
        "os.makedirs(virt_directory, exist_ok=True)\n",
        "print(f\"Directory '{virtpath}' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7vHDrFrHILJi",
      "metadata": {
        "id": "7vHDrFrHILJi"
      },
      "source": [
        "## 1.5 Download data from Zenodo\n",
        "\n",
        "Participants can also use this code to download the OpenPack dataset.\n",
        "\n",
        "\n",
        "---\n",
        "## 1.5 Zenodo からデータをダウンロード\n",
        "\n",
        "参加者はこのコードを使用して OpenPack データセットをダウンロードすることができます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IaUSDHTOIQRj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaUSDHTOIQRj",
        "outputId": "6e4744cd-7acc-4547-f6b6-37d4466c3302"
      },
      "outputs": [],
      "source": [
        "# Construct the URL to the Zenodo API\n",
        "api_url = f\"https://zenodo.org/records/11059235\"\n",
        "\n",
        "# Send a request to the Zenodo API\n",
        "response = requests.get(api_url)\n",
        "response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "# # Parse the JSON response\n",
        "# data = response.json()\n",
        "\n",
        "# Extract the file information\n",
        "download_url = f\"https://zenodo.org/records/11059235/files/imu-with-operation-action-labels.zip?download=1\"\n",
        "\n",
        "# Download the file\n",
        "file_response = requests.get(download_url)\n",
        "file_response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "# Save the file\n",
        "file_path = os.path.join(real_directory, 'imu-with-operation-action-labels.zip')\n",
        "with open(file_path, 'wb') as f:\n",
        "    f.write(file_response.content)\n",
        "\n",
        "print(f\"Downloaded to {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a7941d4d94b239",
      "metadata": {
        "collapsed": false,
        "id": "30a7941d4d94b239"
      },
      "source": [
        "## 1.6 Unzip OpenPack dataset\n",
        "\n",
        "After placing the OpenPack dataset at the '/data/real/' folder, unzip the files and delete the zip files.\n",
        "\n",
        "---\n",
        "## 1.6 OpenPack データセットを解凍する\n",
        "\n",
        "OpenPack データセットを '/data/real/' フォルダに配置した後、ファイルを解凍し、zip ファイルを削除します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9a758fa4d9d166",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:40:11.083311800Z",
          "start_time": "2024-11-26T11:40:00.694264800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd9a758fa4d9d166",
        "outputId": "9e3b9abc-b188-464d-a63f-dfc1ee4d81c0"
      },
      "outputs": [],
      "source": [
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(real_directory):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(real_directory, filename)\n",
        "\n",
        "    # Check if the file is a zip file\n",
        "    if filename.endswith('.zip'):\n",
        "        # Open the zip file\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Extract all contents of the zip file into the directory\n",
        "            zip_ref.extractall(file_path[:-4])\n",
        "            print(f\"Extracted: {filename}\")\n",
        "\n",
        "        # Delete the zip file\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {filename}\")\n",
        "\n",
        "print(\"All zip files have been processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5a8f3aead7a511",
      "metadata": {
        "collapsed": false,
        "id": "7a5a8f3aead7a511"
      },
      "source": [
        "# 2. Use Real Data to Generate Virtual Data\n",
        "\n",
        "Firstly, we randomly split the real data into a training set, validation set, and test set according to a specific ratio. Participants can then use the training set to generate virtual data. Finally, we train the network using both the training set and the virtual data, and we calculate the F1 score on the test set.\n",
        "\n",
        "The following code provides an example of generating virtual data from the training set. Note that: (1) The model structure is fixed and unchanged. (2) The split ratio for the training and test sets, as well as the random seed, will differ from the current settings, which requires the virtual data generation algorithm to be robust against varying data. (3) Participants are free to design data generation algorithms and save them to a specified path: '/data/virtual/', but the size of the virtual data is limited to 500MB.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. 実データを使用して仮想データを生成する\n",
        "\n",
        "まず、実データを特定の比率に従ってトレーニングセット、検証セット、テストセットにランダムに分割します。参加者はトレーニングセットを使用して仮想データを生成できます。最後に、トレーニングセットと仮想データの両方を使用してネットワークをトレーニングし、テストセットの F1 スコアを計算します。\n",
        "\n",
        "次のコードは、トレーニング セットから仮想データを生成する例を示しています。次の点に注意してください。\n",
        "\n",
        "(1) モデル構造は固定されており、変更することはできません。\n",
        "\n",
        "(2) トレーニングセットとテストセットの分割比率、およびランダムシードは現在の設定とは異なります。そのため、仮想データ生成アルゴリズムは変化するデータに対して堅牢である必要があります。\n",
        "\n",
        "(3) 参加者はデータ生成アルゴリズムを自由に設計し、指定されたパス「/data/virtual/」に保存できますが、仮想データのサイズは500MBに制限されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba365afde03f932",
      "metadata": {
        "collapsed": false,
        "id": "ba365afde03f932"
      },
      "source": [
        "## 2.1 Assign train users, validation users, and test users\n",
        "\n",
        "In the OpenPack dataset, U0xxx corresponds to user IDs, and S0xxx corresponds to different experiment settings.\n",
        "\n",
        "In this Challenge, we will only select training (real) and test data from S0100.\n",
        "\n",
        "---\n",
        "## 2.1 トレーニング ユーザー、検証ユーザー、テスト ユーザーを割り当てる\n",
        "\n",
        "OpenPack データセットでは、U0xxx はユーザー ID に対応し、S0xxx はさまざまな実験設定に対応します。\n",
        "\n",
        "このチャレンジでは、S0100 からトレーニング (実際の) データとテスト データのみを選択します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92d0517e92b247c",
      "metadata": {
        "collapsed": false,
        "id": "b92d0517e92b247c"
      },
      "source": [
        "## 2.2 Filter out un-used data\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 未使用のデータを除外する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c071d8483f0a0c41",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:58:21.525546600Z",
          "start_time": "2024-11-26T11:58:21.495067600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c071d8483f0a0c41",
        "outputId": "4d9f4914-5c6d-4fc4-fcba-2be1c9e53704"
      },
      "outputs": [],
      "source": [
        "user_paths = {}\n",
        "for root, dirs, files in os.walk(real_directory):\n",
        "    for file in files:\n",
        "        if file.endswith('S0100.csv'):\n",
        "            user_paths[file[:-10]] = os.path.join(root, file)\n",
        "        else:\n",
        "          os.remove(os.path.join(root, file))  # remove unused data\n",
        "for u, d in user_paths.items():\n",
        "    print('%s at: %s'% (u,d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e088a3db85d2fb82",
      "metadata": {
        "id": "e088a3db85d2fb82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fdd3fbfea9d0344",
      "metadata": {
        "collapsed": false,
        "id": "fdd3fbfea9d0344"
      },
      "source": [
        "## 2.3 Split users to train, validation, and test sets\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 ユーザーをトレーニング、検証、テストセットに分割する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9df5a7359851e8c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T12:16:52.484542700Z",
          "start_time": "2024-11-26T12:16:52.450090200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9df5a7359851e8c",
        "outputId": "346feb00-d758-4bec-b5da-86f78ceeede5"
      },
      "outputs": [],
      "source": [
        "userIDs = list(user_paths.keys())\n",
        "\n",
        "# Shuffle the list to ensure randomness\n",
        "random.shuffle(userIDs)\n",
        "\n",
        "# Calculate the split indices\n",
        "total_length = len(userIDs)\n",
        "train_size = int(total_length * splits[0])  # 70% of 10\n",
        "val_size = int(total_length * splits[1])  # 10% of 10\n",
        "test_size = total_length - train_size - val_size  # 20% of 10\n",
        "\n",
        "# Split the list according to the calculated sizes\n",
        "train_users = np.sort(userIDs[:train_size])      # First 70%\n",
        "val_users = np.sort(userIDs[train_size:train_size + val_size])  # Next 10%\n",
        "test_users = np.sort(userIDs[train_size + val_size:])  # Last 20%\n",
        "\n",
        "print('Training set: %s'%train_users)\n",
        "print('Validation set: %s'%val_users)\n",
        "print('Test set: %s'%test_users)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de07e6c2e80afb9",
      "metadata": {
        "collapsed": false,
        "id": "1de07e6c2e80afb9"
      },
      "source": [
        "## 2.4 Load data according to userIDs\n",
        "\n",
        "Load data of every user as dataframe.\n",
        "Use acceleration data of both wrists only;\n",
        "Use operation label.\n",
        "\n",
        "---\n",
        "## 2.4 ユーザーID に従ってデータをロードします\n",
        "\n",
        "すべてのユーザーのデータをデータフレームとしてロードします。\n",
        "両手首の加速度データのみを使用します。\n",
        "操作ラベルを使用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfbcc63914ccaf29",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T12:34:50.434498700Z",
          "start_time": "2024-11-26T12:34:45.441242400Z"
        },
        "id": "cfbcc63914ccaf29"
      },
      "outputs": [],
      "source": [
        "# selected_columns = ['atr01/acc_x','atr01/acc_y','atr01/acc_z','atr02/acc_x','atr02/acc_y','atr02/acc_z','timestamp','operation']\n",
        "train_data_dict = {}\n",
        "for u in train_users:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    train_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)\n",
        "\n",
        "val_data_dict = {}\n",
        "for u in val_users:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    val_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)\n",
        "\n",
        "test_data_dict = {}\n",
        "for u in test_users:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    test_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fe49aa9ed13dcb",
      "metadata": {
        "collapsed": false,
        "id": "97fe49aa9ed13dcb"
      },
      "source": [
        "## 2.6 Virtual Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18172d2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def delete_csv_files(folder_path):\n",
        "    \"\"\"\n",
        "    Deletes all CSV files in the specified folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path to the folder containing the CSV files.\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
        "    for file_path in csv_files:\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            print(f\"Deleted: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aada8e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_folder_size(folder):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.exists(fp):  # Skip broken symlinks\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return total_size\n",
        "\n",
        "def check_virtual_volume(rootdir, virtpath, limit_mb=500):\n",
        "    \"\"\"\n",
        "    Check if the total size of the virtual data folder stays within limit_mb megabytes.\n",
        "    \n",
        "    Parameters:\n",
        "      rootdir (str): The base root directory.\n",
        "      virtpath (str): The virtual data folder path relative to rootdir.\n",
        "      limit_mb (int): The size limit in MB (default is 500 MB).\n",
        "    \"\"\"\n",
        "    virt_directory = rootdir + virtpath\n",
        "    total_virtual_size = get_folder_size(virt_directory)\n",
        "    limit = limit_mb * 1024 * 1024  # convert MB to bytes\n",
        "\n",
        "    if total_virtual_size <= limit:\n",
        "        # print(f\"合計サイズは {total_virtual_size/(1024**2):.2f} MB で、{limit_mb}MB以下です。\")\n",
        "        return True\n",
        "    else:\n",
        "        # print(f\"合計サイズは {total_virtual_size/(1024**2):.2f} MB で、{limit_mb}MBを超えています。\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2967423ce58fab9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:15:04.268329100Z",
          "start_time": "2024-11-26T11:15:04.258405600Z"
        },
        "id": "2967423ce58fab9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import interp1d, Rbf, PchipInterpolator, Akima1DInterpolator\n",
        "from scipy.interpolate import splrep, splev\n",
        "from scipy.signal import savgol_filter\n",
        "import pywt\n",
        "from pykalman import KalmanFilter\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from delete import delete_csv_files\n",
        "from volumeChecker import check_virtual_volume\n",
        "from scipy.interpolate import interp1d, CubicSpline\n",
        "import numpy as np\n",
        "from scipy.signal.windows import blackmanharris\n",
        "from scipy.special import gamma\n",
        "explanatory_columns = ['atr01/acc_x','atr01/acc_y','atr01/acc_z',\n",
        "                       'atr02/acc_x','atr02/acc_y','atr02/acc_z']\n",
        "# ---------------------------\n",
        "# 各 (operation, action) のセグメント長分布を計算\n",
        "# ---------------------------\n",
        "segment_length_dist = {}  # キーは (operation, action) タプル\n",
        "raw_segment_length_dist = {}  # 外れ値除去前の元データも保存しておく\n",
        "\n",
        "for u, df in train_data_dict.items():\n",
        "    df_temp = df.copy()\n",
        "    # operationまたはactionが変化したら新しいグループとする\n",
        "    df_temp['group'] = ((df_temp['operation'] != df_temp['operation'].shift()) | \n",
        "                        (df_temp['action'] != df_temp['action'].shift())).cumsum()\n",
        "    for _, group_df in df_temp.groupby('group'):\n",
        "        key = (group_df['operation'].iloc[0], group_df['action'].iloc[0])\n",
        "        seg_len = len(group_df)\n",
        "        raw_segment_length_dist.setdefault(key, []).append(seg_len)\n",
        "\n",
        "# IQRを使用して外れ値を除外する\n",
        "for key, lengths in raw_segment_length_dist.items():\n",
        "    lengths_arr = np.array(lengths)\n",
        "    \n",
        "    # IQR計算\n",
        "    q1 = np.percentile(lengths_arr, 25)\n",
        "    q3 = np.percentile(lengths_arr, 75)\n",
        "    iqr = q3 - q1\n",
        "    \n",
        "    # 外れ値の境界を定義 (一般的に使われる1.5*IQRを使用)\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    \n",
        "    # 外れ値でないデータのみをフィルタリング\n",
        "    filtered_lengths = lengths_arr[(lengths_arr >= lower_bound) & (lengths_arr <= upper_bound)]\n",
        "    \n",
        "    # 外れ値を除いたデータを保存\n",
        "    segment_length_dist[key] = filtered_lengths.tolist()\n",
        "\n",
        "# 各セグメントの統計量を出力（デバッグ用）\n",
        "for key, lengths in segment_length_dist.items():\n",
        "    lengths_arr = np.array(lengths)\n",
        "    raw_lengths = np.array(raw_segment_length_dist[key])\n",
        "    \n",
        "    # フィルタリング前後の統計情報を出力\n",
        "    print(f\"Operation {key[0]}, Action {key[1]}: \")\n",
        "    print(f\"  Raw: count={len(raw_lengths)}, mean={raw_lengths.mean():.2f}, median={np.median(raw_lengths):.2f}, std={raw_lengths.std():.2f}\")\n",
        "    print(f\"  Filtered: count={len(lengths_arr)}, mean={lengths_arr.mean():.2f}, median={np.median(lengths_arr):.2f}, std={lengths_arr.std():.2f}\")\n",
        "    print(f\"  Removed: {len(raw_lengths) - len(lengths_arr)} outliers\")\n",
        "\n",
        "# ---------------------------\n",
        "# 補助関数の定義\n",
        "# ---------------------------\n",
        "def generate_unique_filename(prefix):\n",
        "    \"\"\"ユーザID等を接頭辞にしてユニークなファイル名を生成する\"\"\"\n",
        "    return f\"{prefix}_{uuid.uuid4().hex}.csv\"\n",
        "\n",
        "# --- 外れ値フィルタリング ---\n",
        "def filter_moving_average(series, window=5):\n",
        "    return series.rolling(window, min_periods=1, center=True).mean()\n",
        "\n",
        "def filter_ransac(series):\n",
        "    x = np.arange(len(series)).reshape(-1, 1)\n",
        "    y = series.values\n",
        "    model = RANSACRegressor()\n",
        "    model.fit(x, y)\n",
        "    y_pred = model.predict(x)\n",
        "    return pd.Series(y_pred, index=series.index)\n",
        "\n",
        "def filter_wavelet(series, wavelet='db1'):\n",
        "    coeff = pywt.wavedec(series, wavelet)\n",
        "    threshold = np.median(np.abs(coeff[-1])) / 0.6745\n",
        "    new_coeff = [pywt.threshold(c, threshold, mode='soft') for c in coeff]\n",
        "    rec = pywt.waverec(new_coeff, wavelet)\n",
        "    rec = rec[:len(series)]\n",
        "    return pd.Series(rec, index=series.index)\n",
        "\n",
        "def filter_kalman(series):\n",
        "    kf = KalmanFilter(initial_state_mean=series.iloc[0], n_dim_obs=1)\n",
        "    state_means, _ = kf.smooth(series.values)\n",
        "    return pd.Series(state_means.flatten(), index=series.index)\n",
        "\n",
        "def filter_savgol(series, window_length=7, polyorder=2):\n",
        "    if window_length >= len(series):\n",
        "        window_length = len(series) if len(series) % 2 == 1 else len(series) - 1\n",
        "    # Ensure polyorder is less than window_length\n",
        "    if polyorder >= window_length:\n",
        "        polyorder = window_length - 1\n",
        "    return pd.Series(savgol_filter(series, window_length=window_length, polyorder=polyorder), index=series.index)\n",
        "\n",
        "def apply_outlier_filter(series, method='none'):\n",
        "    if method == 'moving_average':\n",
        "        return filter_moving_average(series)\n",
        "    elif method == 'ransac':\n",
        "        return filter_ransac(series)\n",
        "    elif method == 'wavelet':\n",
        "        return filter_wavelet(series)\n",
        "    elif method == 'kalman':\n",
        "        return filter_kalman(series)\n",
        "    elif method == 'savgol':\n",
        "        return filter_savgol(series)\n",
        "    elif method == 'none':\n",
        "        return series\n",
        "    else:\n",
        "        return series\n",
        "\n",
        "# --- 補間手法の定義 ---\n",
        "def interpolate_linear(x, y, new_x):\n",
        "    # Replace non-finite values in y with linearly interpolated values if any\n",
        "    if not np.all(np.isfinite(y)):\n",
        "        y = pd.Series(y).interpolate(method='linear', limit_direction='both').values\n",
        "    f = interp1d(x, y, kind='linear', fill_value=\"extrapolate\")\n",
        "    return f(new_x)\n",
        "\n",
        "def interpolate_rbf(x, y, new_x, function='multiquadric'):\n",
        "    # Replace non-finite values in y with linearly interpolated values if any\n",
        "    if not np.all(np.isfinite(y)):\n",
        "        y = pd.Series(y).interpolate(method='linear', limit_direction='both').values\n",
        "    rbf_func = Rbf(x, y, function=function)\n",
        "    return rbf_func(new_x)\n",
        "\n",
        "def interpolate_bspline(x, y, new_x, k=3, s=0):\n",
        "    if len(x) <= k:\n",
        "        return np.interp(new_x, x, y)\n",
        "    tck = splrep(x, y, k=k, s=s)\n",
        "    return splev(new_x, tck)\n",
        "\n",
        "def interpolate_akima(x, y, new_x):\n",
        "    # Replace non-finite values in y with linearly interpolated values if any\n",
        "    if not np.all(np.isfinite(y)):\n",
        "        y = pd.Series(y).interpolate(method='linear', limit_direction='both').values\n",
        "    try:\n",
        "        akima = Akima1DInterpolator(x, y)\n",
        "        return akima(new_x)\n",
        "    except ValueError:\n",
        "        return np.interp(new_x, x, y)\n",
        "\n",
        "def interpolate_pchip(x, y, new_x):\n",
        "    # Replace non-finite values in y with linearly interpolated values if any\n",
        "    if not np.all(np.isfinite(y)):\n",
        "        y = pd.Series(y).interpolate(method='linear', limit_direction='both').values\n",
        "    pchip = PchipInterpolator(x, y)\n",
        "    return pchip(new_x)\n",
        "\n",
        "def perform_interpolation(x, y, new_x, method='pchip', **kwargs):\n",
        "    x = np.asarray(x)\n",
        "    y = np.asarray(y)\n",
        "    # Filtering NaNなど必要ならここで処理する\n",
        "    if method == 'interpolate':\n",
        "        return interpolate_linear(x, y, new_x)\n",
        "    elif method == 'rbf_inverse':\n",
        "        return interpolate_rbf(x, y, new_x, function='inverse')\n",
        "    elif method == 'rbf_multiquadric':\n",
        "        return interpolate_rbf(x, y, new_x, function='multiquadric')\n",
        "    elif method == 'rbf_gaussian':\n",
        "        return interpolate_rbf(x, y, new_x, function='gaussian')\n",
        "    elif method == 'rbf_linear':\n",
        "        return interpolate_rbf(x, y, new_x, function='linear')\n",
        "    elif method == 'rbf_cubic':\n",
        "        return interpolate_rbf(x, y, new_x, function='cubic')\n",
        "    elif method == 'bspline':\n",
        "        return interpolate_bspline(x, y, new_x)\n",
        "    elif method == 'fft_hann':\n",
        "        return interpolate_fft(x, y, new_x, window='hann')\n",
        "    elif method == 'fft_Welch':\n",
        "        return interpolate_fft(x, y, new_x, window='Welch')\n",
        "    elif method == 'fft_Blackman-Harris':\n",
        "        return interpolate_fft(x, y, new_x, window='Blackman-Harris')\n",
        "    elif method == 'akima':\n",
        "        return interpolate_akima(x, y, new_x)\n",
        "    elif method == 'pchip':\n",
        "        return interpolate_pchip(x, y, new_x)\n",
        "    elif method == \"hermite\":\n",
        "        # Use PCHIP (Piecewise Cubic Hermite Interpolating Polynomial) for Hermite interpolation\n",
        "        pchip = PchipInterpolator(x, y)\n",
        "        return pchip(new_x)\n",
        "    elif method == 'hybrid':\n",
        "        return hybrid_interpolate(x, y, new_x)\n",
        "    else:\n",
        "        return interpolate_linear(x, y, new_x)\n",
        "\n",
        "def hybrid_interpolate(time, values, new_x, max_linear_gap=5):\n",
        "    \"\"\"\n",
        "    Interpolate missing values (NaNs) in a time series using a hybrid approach:\n",
        "    - Linear interpolation for gaps shorter or equal to max_linear_gap.\n",
        "    - Cubic spline interpolation for larger gaps.\n",
        "    After filling missing values on the original time axis, the result is interpolated onto new_x.\n",
        "    \"\"\"\n",
        "    values = np.asarray(values).copy()\n",
        "    n = len(values)\n",
        "    isnan = np.isnan(values)\n",
        "    if not np.any(isnan):\n",
        "        if len(new_x) != n:\n",
        "            return np.interp(new_x, time, values)\n",
        "        return values\n",
        "    \n",
        "    # Find gaps (continuous NaN segments) by scanning through data\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        if np.isnan(values[i]):\n",
        "            start = i\n",
        "            while i < n and np.isnan(values[i]):\n",
        "                i += 1\n",
        "            end = i\n",
        "            gap_length = end - start\n",
        "            if gap_length <= max_linear_gap:\n",
        "                if start == 0 or end == n:\n",
        "                    continue\n",
        "                x0, x1 = time[start-1], time[end]\n",
        "                y0, y1 = values[start-1], values[end]\n",
        "                interp_times = time[start:end]\n",
        "                values[start:end] = y0 + (y1 - y0) * ((interp_times - x0) / (x1 - x0))\n",
        "            else:\n",
        "                if start == 0 or end == n:\n",
        "                    if start == 0 and end < n:\n",
        "                        values[start:end] = values[end]\n",
        "                    elif end == n and start > 0:\n",
        "                        values[start:end] = values[start-1]\n",
        "                    continue\n",
        "                idx_before = start - 1\n",
        "                idx_after = end\n",
        "                spline_x = time[[idx_before, idx_after]]\n",
        "                spline_y = values[[idx_before, idx_after]]\n",
        "                cs = CubicSpline(spline_x, spline_y, bc_type='natural')\n",
        "                interp_times = time[start:end]\n",
        "                values[start:end] = cs(interp_times)\n",
        "        else:\n",
        "            i += 1\n",
        "    if len(new_x) != n:\n",
        "        values = np.interp(new_x, time, values)\n",
        "    return values\n",
        "def interpolate_fft(x, y, new_x, window='hann'):\n",
        "    \"\"\"\n",
        "    FFTベースの補間を実施する関数\n",
        "\n",
        "    Parameters:\n",
        "      x : array-like\n",
        "          元のタイムスタンプ（均等サンプリングされている前提）\n",
        "      y : array-like\n",
        "          補間対象のデータ系列\n",
        "      new_x : array-like\n",
        "          補間後のタイムスタンプ。補間結果は新たに一様な系列として得られるが、\n",
        "          new_xが元の一様系列と異なる場合、線形補間により最終的な値を算出する。\n",
        "      window : str, optional\n",
        "          使用する窓関数。'hann'、'Welch'、'Blackman-Harris'、もしくはその他（デフォルトは'hann'）。\n",
        "    \n",
        "    Returns:\n",
        "      y_final : array-like\n",
        "          new_xに対応する補間後のデータ系列\n",
        "    \"\"\"\n",
        "    # numpy配列に変換\n",
        "    x = np.asarray(x)\n",
        "    y = np.asarray(y)\n",
        "    new_x = np.asarray(new_x)\n",
        "    \n",
        "    N = len(y)\n",
        "    new_N = len(new_x)\n",
        "    \n",
        "    # --- 窓関数の生成 ---\n",
        "    if window == 'hann':\n",
        "        win = np.hanning(N)\n",
        "    elif window == 'Welch':\n",
        "        # Welch窓はパラボリック窓とも呼ばれる\n",
        "        n = np.arange(N)\n",
        "        win = 1 - ((n - (N-1)/2) / ((N-1)/2))**2\n",
        "    elif window == 'Blackman-Harris':\n",
        "        win = blackmanharris(N)\n",
        "    else:\n",
        "        win = np.ones(N)\n",
        "    \n",
        "    # 入力系列に窓を適用\n",
        "    y_windowed = y * win\n",
        "\n",
        "    # --- FFTを計算 ---\n",
        "    Y = np.fft.fft(y_windowed)\n",
        "    \n",
        "    # --- ゼロパディングまたはトランケーション ---\n",
        "    if new_N > N:\n",
        "        pad = new_N - N\n",
        "        # FFT係数の左右対称性を保つため、中央にゼロを挿入\n",
        "        if N % 2 == 0:\n",
        "            left = Y[:N//2]\n",
        "            right = Y[N//2:]\n",
        "        else:\n",
        "            left = Y[:(N+1)//2]\n",
        "            right = Y[(N+1)//2:]\n",
        "        Y_padded = np.concatenate([left, np.zeros(pad, dtype=complex), right])\n",
        "    else:\n",
        "        # 補間先のサイズが小さい場合は先頭new_N個を採用（必要に応じた実装変更も可）\n",
        "        Y_padded = Y[:new_N]\n",
        "    \n",
        "    # --- 逆FFTにより時系列信号に戻す ---\n",
        "    y_interp = np.fft.ifft(Y_padded)\n",
        "    # 実部を取り、スケーリング（新旧データ数の比率）で調整\n",
        "    y_interp = np.real(y_interp) * (float(new_N) / N)\n",
        "    \n",
        "    # --- FFT補間結果は一様なタイム軸上のデータとなる ---\n",
        "    t_uniform = np.linspace(x[0], x[-1], new_N)\n",
        "    \n",
        "    # new_xが一様なタイム軸と一致しない場合は、線形補間で最終調整\n",
        "    if not np.allclose(new_x, t_uniform):\n",
        "        y_final = np.interp(new_x, t_uniform, y_interp)\n",
        "    else:\n",
        "        y_final = y_interp\n",
        "\n",
        "    return y_final\n",
        "\n",
        "\n",
        "def dtw_align_paths(seq1, seq2, dist=lambda x, y: abs(x - y)):\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    dtw = np.full((n+1, m+1), np.inf)\n",
        "    dtw[0, 0] = 0\n",
        "    ptr = np.zeros((n+1, m+1, 2), dtype=int)\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            cost = dist(seq1[i-1], seq2[j-1])\n",
        "            choices = [dtw[i-1, j], dtw[i, j-1], dtw[i-1, j-1]]\n",
        "            idx = np.argmin(choices)\n",
        "            dtw[i, j] = cost + choices[idx]\n",
        "            if idx == 0:\n",
        "                ptr[i, j] = [i-1, j]\n",
        "            elif idx == 1:\n",
        "                ptr[i, j] = [i, j-1]\n",
        "            else:\n",
        "                ptr[i, j] = [i-1, j-1]\n",
        "    i, j = n, m\n",
        "    path = []\n",
        "    while i > 0 or j > 0:\n",
        "        path.append((i-1, j-1))\n",
        "        i_prev, j_prev = ptr[i, j]\n",
        "        i, j = i_prev, j_prev\n",
        "    path.reverse()\n",
        "    return path\n",
        "\n",
        "# ---------------------------\n",
        "# 仮想データ生成処理\n",
        "# ---------------------------\n",
        "def process_csv_files(output_dir, num_timestamps=None,\n",
        "                      outlier_method='moving_average', \n",
        "                      interpolation_method='bspline',\n",
        "                      round_values=False,\n",
        "                      distribution_type='normal'):\n",
        "    \"\"\"\n",
        "    指定した出力ディレクトリに、trainデータから生成した仮想データCSVを出力する。\n",
        "    手順:\n",
        "      1. train_data_dict内の各CSVデータについて、operationおよびactionの変化ごとにセグメントを抽出\n",
        "         （セグメント長分布は事前に計算した分布を利用）\n",
        "      2. 各セグメントのtimestampを先頭0起点に調整\n",
        "      3. 指定の外れ値除去法で加速度データをフィルタリング\n",
        "      4. 指定の補間手法で各セグメントを補間し、新しいタイムスタンプ系列にリサンプリング\n",
        "         - num_timestamps=Trueの場合、各セグメントの長さを、(operation, action)ごとの実データ分布からランダムにサンプリングする\n",
        "         - num_timestamps=Noneの場合は元のtimestamp系列をそのまま利用\n",
        "      5. 必要に応じて値を丸め（round_values）\n",
        "      6. CSVとして出力し、出力ディレクトリの容量がlimit_mb未満なら処理を継続\n",
        "    \n",
        "    Args:\n",
        "        distribution_type: 生成するセグメント長の分布タイプ ('normal', 'gamma', 'exponential')\n",
        "    \"\"\"\n",
        "    # 出力先内のCSVをすべて削除\n",
        "    delete_csv_files(output_dir)\n",
        "    # ループ：出力ディレクトリ容量が500MB未満の場合に生成（容量超えたら終了）\n",
        "    while check_virtual_volume(rootdir, virtpath, limit_mb=500) is True:\n",
        "        for user, df in train_data_dict.items():\n",
        "            if check_virtual_volume(rootdir, virtpath, limit_mb=500) is False:\n",
        "                return None\n",
        "            df_proc = df[selected_columns].copy()\n",
        "            # operationまたはactionが変化したら新グループとする\n",
        "            df_proc['group'] = ((df_proc['operation'] != df_proc['operation'].shift()) |\n",
        "                                (df_proc['action'] != df_proc['action'].shift())).cumsum()\n",
        "            processed_groups = []\n",
        "            for group_id, group_df in df_proc.groupby('group'):\n",
        "                group_df = group_df.copy()\n",
        "                # タイムスタンプを先頭0起点に調整\n",
        "                group_df['timestamp'] = group_df['timestamp'] - group_df['timestamp'].iloc[0]\n",
        "                \n",
        "                # 新しいtimestamp系列の生成\n",
        "                if num_timestamps:\n",
        "                    op_val = group_df['operation'].iloc[0]\n",
        "                    act_val = group_df['action'].iloc[0]\n",
        "                    key = (op_val, act_val)\n",
        "                    if key in segment_length_dist:\n",
        "                        durations = np.array(segment_length_dist[key])\n",
        "                        mean_val = durations.mean()\n",
        "                        std_val = durations.std()\n",
        "                        \n",
        "                        # 選択された分布タイプに基づいてサンプリング\n",
        "                        if distribution_type == 'normal':\n",
        "                            current_num = int(np.round(np.random.normal(mean_val, std_val)))\n",
        "                        elif distribution_type == 'gamma':\n",
        "                            # ガンマ分布パラメータ: シェイプ k とスケール θ を推定\n",
        "                            # 正規分布の平均と分散からガンマ分布のパラメータを推定\n",
        "                            if mean_val > 0 and std_val > 0:\n",
        "                                shape = (mean_val / std_val) ** 2\n",
        "                                scale = std_val ** 2 / mean_val\n",
        "                                current_num = int(np.round(np.random.gamma(shape, scale)))\n",
        "                            else:\n",
        "                                current_num = int(mean_val)\n",
        "                        elif distribution_type == 'exponential':\n",
        "                            # 指数分布のパラメータ λ = 1/平均\n",
        "                            if mean_val > 0:\n",
        "                                scale = mean_val  # 指数分布の scale=1/rate\n",
        "                                current_num = int(np.round(np.random.exponential(scale)))\n",
        "                            else:\n",
        "                                current_num = int(mean_val)\n",
        "                        elif distribution_type == 'lognormal':\n",
        "                            # 対数正規分布のパラメータ μ と σ を推定\n",
        "                            if mean_val > 0 and std_val > 0:\n",
        "                                var = std_val ** 2\n",
        "                                mu = np.log(mean_val**2 / np.sqrt(var + mean_val**2))\n",
        "                                sigma = np.sqrt(np.log(1 + var / mean_val**2))\n",
        "                                current_num = int(np.round(np.random.lognormal(mu, sigma)))\n",
        "                            else:\n",
        "                                current_num = int(mean_val)\n",
        "                        elif distribution_type == 'weibull':\n",
        "                            # ワイブル分布のパラメータ k と λ を推定\n",
        "                            # 形状パラメータ k は1.2（やや右に裾が長い）、スケールパラメータはmean_valを基準に調整\n",
        "                            if mean_val > 0:\n",
        "                                k = 1.2  # 形状パラメータ\n",
        "                                # Γ(1 + 1/k) = ガンマ関数\n",
        "                                lambda_param = mean_val / gamma(1 + 1/k)\n",
        "                                current_num = int(np.round(np.random.weibull(k) * lambda_param))\n",
        "                            else:\n",
        "                                current_num = int(mean_val)\n",
        "                        elif distribution_type == 'poisson':\n",
        "                            # ポアソン分布（整数値を生成する）\n",
        "                            if mean_val > 0:\n",
        "                                current_num = np.random.poisson(mean_val)\n",
        "                            else:\n",
        "                                current_num = 1\n",
        "                        else:\n",
        "                            # デフォルトは正規分布\n",
        "                            current_num = int(np.round(np.random.normal(mean_val, std_val)))\n",
        "                            \n",
        "                        current_num = max(current_num, 1)\n",
        "                    else:\n",
        "                        current_num = max(len(group_df), 1)\n",
        "                    new_timestamps = np.linspace(group_df['timestamp'].min(),\n",
        "                                                 group_df['timestamp'].max(),\n",
        "                                                 current_num)\n",
        "                else:\n",
        "                    new_timestamps = group_df['timestamp'].values\n",
        "                \n",
        "                # 新しいグループDataFrameを作成（補間後の行数 = new_timestampsの長さ）\n",
        "                new_group_df = pd.DataFrame({'timestamp': new_timestamps})\n",
        "                # グループ内の行数が1なら、外れ値フィルタや補間処理をスキップ\n",
        "                if len(group_df) == 1:\n",
        "                    if num_timestamps is not None:\n",
        "                        # num_timestamps が指定されていれば、新しいDataFrameを作成する\n",
        "                        ts_min = group_df['timestamp'].min()\n",
        "                        ts_max = group_df['timestamp'].max()\n",
        "                        new_timestamps = np.linspace(ts_min, ts_max, num_timestamps)\n",
        "                        # 元のデータを複製して新しいDataFrameを作成\n",
        "                        new_df = pd.DataFrame([group_df.iloc[0].to_dict()] * num_timestamps)\n",
        "                        new_df['timestamp'] = new_timestamps\n",
        "                        processed_groups.append(new_df)\n",
        "                    else:\n",
        "                        processed_groups.append(group_df)\n",
        "                    continue\n",
        "                # 各説明変数に対してフィルタと補間を実施\n",
        "                for col in explanatory_columns:\n",
        "                    filtered_series = apply_outlier_filter(group_df[col], method=outlier_method)\n",
        "                    interp_values = perform_interpolation(group_df['timestamp'].values,\n",
        "                                                          filtered_series.values,\n",
        "                                                          new_timestamps,\n",
        "                                                          method=interpolation_method)\n",
        "                    new_group_df[col] = interp_values\n",
        "                # operation列を付与（actionは評価には使わないので省略可）\n",
        "                new_group_df['operation'] = group_df['operation'].iloc[0]\n",
        "                processed_groups.append(new_group_df)\n",
        "            final_df = pd.concat(processed_groups, ignore_index=True)\n",
        "            if round_values:\n",
        "                final_df = final_df.round(5)\n",
        "            if 'timestamp' in final_df.columns:\n",
        "                final_df = final_df.drop(columns=['timestamp'])\n",
        "            if 'action' in final_df.columns:\n",
        "                final_df = final_df.drop(columns=['action'])\n",
        "            if 'group' in final_df.columns:\n",
        "                final_df = final_df.drop(columns=['group'])\n",
        "            output_file = os.path.join(output_dir, generate_unique_filename(user))\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            if check_virtual_volume(rootdir, virtpath, limit_mb=500) is False:\n",
        "                return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WDy12Am-ceS-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDy12Am-ceS-",
        "outputId": "c5d1e92b-2d1a-412e-8c95-ae73a761c0fc"
      },
      "outputs": [],
      "source": [
        "def get_folder_size(folder_path):\n",
        "    total_size = 0\n",
        "    # Walk through all files and subdirectories in the folder\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            # Get the full file path\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            # Add the size of each file to the total size\n",
        "            total_size += os.path.getsize(file_path)\n",
        "    return total_size\n",
        "\n",
        "# Example usage\n",
        "size = get_folder_size(virt_directory)\n",
        "print(f\"The total size of the folder is: {size} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c18c2a90cf62034",
      "metadata": {
        "collapsed": false,
        "id": "2c18c2a90cf62034"
      },
      "source": [
        "# 3. Use the Generated Data to Improve HAR Model Performance\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 3. 生成されたデータを使用してHARモデルのパフォーマンスを向上させる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6356dfab9eb38848",
      "metadata": {
        "collapsed": false,
        "id": "6356dfab9eb38848"
      },
      "source": [
        "## 3.1 Read data and labels from both real and virtual folders\n",
        "\n",
        "\n",
        "---\n",
        "## 3.1 実フォルダと仮想フォルダの両方からデータとラベルを読み取る\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52a01881aa3e839",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:06:15.505458200Z",
          "start_time": "2024-11-26T17:06:15.440409700Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52a01881aa3e839",
        "outputId": "bd831f67-89d0-4c0e-8ae1-18682c18372c"
      },
      "outputs": [],
      "source": [
        "# find csv files in 'data/virtual'\n",
        "virt_paths = []\n",
        "for root, dirs, files in os.walk(virt_directory):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            virt_paths.append(os.path.join(root, file))\n",
        "print('Virtual csv file paths are as shown follows:')\n",
        "virt_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317fd3932804be3e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:06:31.374298Z",
          "start_time": "2024-11-26T17:06:29.371508900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "317fd3932804be3e",
        "outputId": "935fa838-384b-4002-f510-7e4fdfcfc6de"
      },
      "outputs": [],
      "source": [
        "# real and virtual training data\n",
        "\n",
        "## real data\n",
        "train_data = []\n",
        "for u, data in train_data_dict.items():\n",
        "    train_data.append(data[new_columns].values)\n",
        "    # print(data[new_columns].values.shape)\n",
        "\n",
        "## virtual data\n",
        "for p in virt_paths:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    data = pd.read_csv(p, usecols=new_columns)\n",
        "    train_data.append(data.values)\n",
        "\n",
        "train_data = np.concatenate(train_data, axis=0)\n",
        "print('Shape of train data is %s'%str(train_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad48585ae2ea52d6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:07:26.403366900Z",
          "start_time": "2024-11-26T17:07:26.246585200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad48585ae2ea52d6",
        "outputId": "4761d4d7-63dd-40e4-83e8-199880840494"
      },
      "outputs": [],
      "source": [
        "# validatation and test data\n",
        "val_data = []\n",
        "for u, data in val_data_dict.items():\n",
        "    val_data.append(data[new_columns].values)\n",
        "\n",
        "test_data = []\n",
        "for u, data in test_data_dict.items():\n",
        "    test_data.append(data[new_columns].values)\n",
        "\n",
        "val_data = np.concatenate(val_data, axis=0)\n",
        "test_data = np.concatenate(test_data, axis=0)\n",
        "\n",
        "print('Shape of validation data is %s'%str(val_data.shape))\n",
        "print('Shape of test data is %s'%str(test_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5853fe098d9073b0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:09:38.449495900Z",
          "start_time": "2024-11-26T17:09:37.884790900Z"
        },
        "id": "5853fe098d9073b0"
      },
      "outputs": [],
      "source": [
        "# convert operation ID to labels (from 0 to n)\n",
        "labels = np.unique(train_data[:, -1])\n",
        "label_dict = dict(zip(labels, np.arange(len(labels))))\n",
        "train_data[:,-1] = np.array([label_dict[i] for i in train_data[:,-1]])\n",
        "val_data[:,-1] =  np.array([label_dict[i] for i in val_data[:,-1]])\n",
        "test_data[:,-1] =  np.array([label_dict[i] for i in test_data[:,-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f7ecdf96bd4fff",
      "metadata": {
        "id": "a4f7ecdf96bd4fff"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9693fd76fa677383",
      "metadata": {
        "collapsed": false,
        "id": "9693fd76fa677383"
      },
      "source": [
        "## 3.2 Prepare Dataloader\n",
        "\n",
        "---\n",
        "\n",
        "## 3.2 データローダーの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5941002ccfa02913",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:10:20.929443300Z",
          "start_time": "2024-11-26T17:10:20.898518300Z"
        },
        "id": "5941002ccfa02913"
      },
      "outputs": [],
      "source": [
        "class data_loader_OpenPack(Dataset):\n",
        "    def __init__(self, samples, labels, device='cpu'):\n",
        "        self.samples = torch.tensor(samples).to(device)  # check data type\n",
        "        self.labels = torch.tensor(labels)  # check data type\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.labels[index]\n",
        "        sample = self.samples[index]\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def sliding_window(datanp, len_sw, step):\n",
        "    '''\n",
        "    :param datanp: shape=(data length, dim) raw sensor data and the labels. The last column is the label column.\n",
        "    :param len_sw: length of the segmented sensor data\n",
        "    :param step: overlapping length of the segmented data\n",
        "    :return: shape=(N, len_sw, dim) batch of sensor data segment.\n",
        "    '''\n",
        "\n",
        "    # generate batch of data by overlapping the training set\n",
        "    data_batch = []\n",
        "    for idx in range(0, datanp.shape[0] - len_sw - step, step):\n",
        "        data_batch.append(datanp[idx: idx + len_sw, :])\n",
        "    data_batch.append(datanp[-1 - len_sw: -1, :])  # last batch\n",
        "    xlist = np.stack(data_batch, axis=0)  # [B, data length, dim]\n",
        "\n",
        "    return xlist\n",
        "\n",
        "def generate_dataloader(data, len_sw, step, if_shuffle=True):\n",
        "    tmp_b = sliding_window(data, len_sw, step)\n",
        "    data_b = tmp_b[:, :, :-1]\n",
        "    label_b = tmp_b[:, :, -1]\n",
        "    data_set_r = data_loader_OpenPack(data_b, label_b, device=device)\n",
        "    data_loader = DataLoader(data_set_r, batch_size=batch_size,\n",
        "                              shuffle=if_shuffle, drop_last=False)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d6e62da61c17d9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:10:24.113448400Z",
          "start_time": "2024-11-26T17:10:23.094960500Z"
        },
        "id": "d7d6e62da61c17d9"
      },
      "outputs": [],
      "source": [
        "len_sw = 300\n",
        "step = 150\n",
        "batch_size = 512\n",
        "\n",
        "train_loader = generate_dataloader(train_data, len_sw, step, if_shuffle=True)\n",
        "val_loader = generate_dataloader(val_data, len_sw, step, if_shuffle=False)\n",
        "test_loader = generate_dataloader(test_data, len_sw, step, if_shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be50fc0f4fdbbb16",
      "metadata": {
        "collapsed": false,
        "id": "be50fc0f4fdbbb16"
      },
      "source": [
        "## 3.3 Prepare Model\n",
        "\n",
        "Reference:\n",
        "https://github.com/Tian0426/CL-HAR\n",
        "\n",
        "---\n",
        "## 3.3 モデルの準備\n",
        "\n",
        "参照:\n",
        "https://github.com/Tian0426/CL-HAR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c655b4c595520ad2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:31:54.694552300Z",
          "start_time": "2024-11-26T17:31:54.672758900Z"
        },
        "id": "c655b4c595520ad2"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value=True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        self.attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', self.attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class Transformer_block(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                (PreNorm(dim, Attention(dim, heads=heads, dropout=dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Seq_Transformer(nn.Module):\n",
        "    def __init__(self, n_channel, len_sw, n_classes, dim=128, depth=4, heads=4, mlp_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_to_embedding = nn.Linear(n_channel, dim)\n",
        "        self.c_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.position = PositionalEncoding(d_model=dim, max_len=len_sw)\n",
        "        self.transformer = Transformer_block(dim, depth, heads, mlp_dim, dropout)\n",
        "        self.to_c_token = nn.Identity()\n",
        "        self.classifier = nn.Linear(dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, forward_seq):\n",
        "        x = self.patch_to_embedding(forward_seq)\n",
        "        x = self.position(x)\n",
        "        b, n, _ = x.shape\n",
        "        c_tokens = repeat(self.c_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((c_tokens, x), dim=1)\n",
        "        x = self.transformer(x)\n",
        "        c_t = self.to_c_token(x[:, 0])\n",
        "        return c_t\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_channels=6, len_sw=300, n_classes=11, dim=128, depth=4, heads=4, mlp_dim=64, dropout=0.3):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.out_dim = dim\n",
        "        self.transformer = Seq_Transformer(n_channel=n_channels, len_sw=len_sw, n_classes=n_classes, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim, dropout=dropout)\n",
        "        self.classifier = nn.Linear(dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer(x)\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "        # return out, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655130025ce06d58",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:47.180838500Z",
          "start_time": "2024-11-26T17:32:47.100693300Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655130025ce06d58",
        "outputId": "a5dc62f5-bf5e-4841-f6e4-5d016a7a75e3"
      },
      "outputs": [],
      "source": [
        "model = Transformer()\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7b7fef2f95d4f0",
      "metadata": {
        "collapsed": false,
        "id": "1c7b7fef2f95d4f0"
      },
      "source": [
        "## 3.4 Training and test\n",
        "\n",
        "Reference:\n",
        "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/main.py\n",
        "\n",
        "---\n",
        "## 3.4 トレーニングとテスト\n",
        "\n",
        "参考:\n",
        "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbc8df0df53ab2d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:49.432241200Z",
          "start_time": "2024-11-26T17:32:49.394337400Z"
        },
        "id": "fcbc8df0df53ab2d"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                if self.verbose:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                return True\n",
        "        return False\n",
        "early_stopping = EarlyStopping()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PZNkw85JqQ4O",
      "metadata": {
        "id": "PZNkw85JqQ4O"
      },
      "outputs": [],
      "source": [
        "def vote_labels(label):\n",
        "    # Iterate over each sample in the batch\n",
        "    votel = []\n",
        "    for i in range(label.size(0)):\n",
        "        # Get unique labels and their counts\n",
        "        unique_labels, counts = label[i].unique(return_counts=True)\n",
        "\n",
        "        # Find the index of the maximum count\n",
        "        max_count_index = counts.argmax()\n",
        "\n",
        "        # Get the label corresponding to that maximum count\n",
        "        mode_label = unique_labels[max_count_index]\n",
        "\n",
        "        # Append the mode to the result list\n",
        "        votel.append(mode_label)\n",
        "\n",
        "    # Convert the result list to a tensor and reshape to (batch, 1)\n",
        "    vote_label = torch.tensor(votel, dtype=torch.long).view(-1)\n",
        "    return vote_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97897e6f2ad9e6cd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:55.039601400Z",
          "start_time": "2024-11-26T17:32:54.946321600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "97897e6f2ad9e6cd",
        "outputId": "bdd74dd4-2414-4b83-fbb4-4315c2a34f3d"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(\n",
        "            model.parameters(), lr=learning_rate, amsgrad=True\n",
        "        )\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18f514279c93576",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:56.222566600Z",
          "start_time": "2024-11-26T17:32:56.048601700Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a18f514279c93576",
        "outputId": "e7c00691-005c-4281-ae04-933fac8e6689"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = [], []\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train_loss, val_loss = [], []\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    true_labels, pred_labels = [], []\n",
        "    for i, (sample, label) in enumerate(train_loader):\n",
        "        sample = sample.to(device=device, dtype=torch.float)\n",
        "        label = label.to(device=device, dtype=torch.long)\n",
        "        vote_label = vote_labels(label)\n",
        "        vote_label = vote_label.to(device)\n",
        "        output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
        "        loss = criterion(output, vote_label)\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        true_labels.append(vote_label.detach().cpu().numpy())\n",
        "        pred_labels.append(output.detach().cpu().numpy())\n",
        "\n",
        "    train_losses.append(np.average(train_loss))\n",
        "    # Calculate F1 scores\n",
        "    y_true = np.concatenate(true_labels, axis=0)\n",
        "    y_prob = np.concatenate(pred_labels, axis=0)\n",
        "\n",
        "    # Get the predicted class labels (argmax along the class dimension)\n",
        "    y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
        "\n",
        "    # Calculate F1 score (macro F1 score)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f'F1 Score of training set: {f1:.4f}')\n",
        "\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        true_labels, pred_labels = [], []\n",
        "        for i, (sample, label) in enumerate(val_loader):\n",
        "            sample = sample.to(device=device, dtype=torch.float)\n",
        "            label = label.to(device=device, dtype=torch.long)\n",
        "            vote_label = vote_labels(label)\n",
        "            vote_label = vote_label.to(device)\n",
        "            output = model(sample)\n",
        "            loss = criterion(output, vote_label)\n",
        "            val_loss.append(loss.item())\n",
        "            true_labels.append(vote_label.detach().cpu().numpy())\n",
        "            pred_labels.append(output.detach().cpu().numpy())\n",
        "        val_losses.append(np.average(val_loss))\n",
        "\n",
        "        # Calculate F1 scores\n",
        "        y_true = np.concatenate(true_labels, axis=0)\n",
        "        y_prob = np.concatenate(pred_labels, axis=0)\n",
        "\n",
        "        # Get the predicted class labels (argmax along the class dimension)\n",
        "        y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
        "\n",
        "        # Calculate F1 score (macro F1 score)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        print(f'F1 Score of validation set: {f1:.4f}')\n",
        "\n",
        "        # Check early stopping\n",
        "        if early_stopping(np.average(val_losses)):\n",
        "            print(\"Stopping at epoch %s.\" % str(epoch))\n",
        "            break\n",
        "    scheduler.step(np.average(val_loss))\n",
        "    # Print the current learning rate\n",
        "    current_lr = scheduler.get_last_lr()[0]  # Get the current learning rate\n",
        "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MuGFwbEeaNN1",
      "metadata": {
        "id": "MuGFwbEeaNN1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(val_losses, label='valid loss')\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6e208b8a744644",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:13:54.424336Z",
          "start_time": "2024-11-26T17:13:54.391946400Z"
        },
        "id": "9b6e208b8a744644"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    true_labels, pred_labels = [], []\n",
        "    for i, (sample, label) in enumerate(test_loader):\n",
        "        sample = sample.to(device=device, dtype=torch.float)\n",
        "        label = label.to(device=device, dtype=torch.long)\n",
        "        vote_label = vote_labels(label)\n",
        "        # vote_label = vote_label.to(device)\n",
        "        output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
        "\n",
        "        true_labels.append(vote_label.numpy())\n",
        "        pred_labels.append(output.detach().cpu().numpy())\n",
        "\n",
        "    # Calculate F1 scores\n",
        "    y_true = np.concatenate(true_labels, axis=0)\n",
        "    y_prob = np.concatenate(pred_labels, axis=0)\n",
        "\n",
        "    # Get the predicted class labels (argmax along the class dimension)\n",
        "    y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
        "\n",
        "    # Calculate F1 score (macro F1 score)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f'F1 Score of test set: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8JL7F0XeeK5",
      "metadata": {
        "id": "V8JL7F0XeeK5"
      },
      "source": [
        "# 4. Submission\n",
        "\n",
        "Participants only need to submit the code related to the virtual data generation.\n",
        "During the evaluation of the challenge, we will call \"custom_virtual_data_generation\" function to generate virtual data. So, please ensure that all relevant codes are included in the .py file.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 4. 提出\n",
        "\n",
        "参加者は仮想データ生成に関連するコードのみを提出する必要があります。\n",
        "チャレンジの評価中に、仮想データを生成するために「custom_virtual_data_generation」関数を呼び出します。ここでは、関連するすべてのコードが .py ファイルに含まれていることを確認してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8YotuUraesYD",
      "metadata": {
        "id": "8YotuUraesYD"
      },
      "outputs": [],
      "source": [
        "# Define the functions you want to save\n",
        "functions_code = \"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def switch_axis(sample, choice):\n",
        "\n",
        "    x = sample[0, :]\n",
        "    y = sample[1, :]\n",
        "    z = sample[2, :]\n",
        "\n",
        "    if choice == 0:\n",
        "        return sample\n",
        "    elif choice == 1:\n",
        "        sample = np.stack([x, y, z], axis=0)\n",
        "    elif choice == 2:\n",
        "        sample = np.stack([x, z, y], axis=0)\n",
        "    elif choice == 3:\n",
        "        sample = np.stack([y, x, z], axis=0)\n",
        "    elif choice == 4:\n",
        "        sample = np.stack([y, z, x], axis=0)\n",
        "    elif choice == 5:\n",
        "        sample = np.stack([z, x, y], axis=0)\n",
        "    elif choice == 6:\n",
        "        sample = np.stack([z, y, x], axis=0)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def flip(sample, choice):\n",
        "\n",
        "    if choice == 1:\n",
        "        sample = np.flip(sample, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
        "    X_new = np.zeros(X.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile is True:\n",
        "        segs = np.zeros(nPerm + 1, dtype=int)\n",
        "        segs[1:-1] = np.sort(\n",
        "            np.random.randint(\n",
        "                minSegLength, X.shape[0] - minSegLength, nPerm - 1\n",
        "            )\n",
        "        )\n",
        "        segs[-1] = X.shape[0]\n",
        "        if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        x_temp = X[segs[idx[ii]] : segs[idx[ii] + 1], :]\n",
        "        X_new[pp : pp + len(x_temp), :] = x_temp\n",
        "        pp += len(x_temp)\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def permute(sample, choice, nPerm=4, minSegLength=10):\n",
        "\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = DA_Permutation(sample, nPerm=nPerm, minSegLength=minSegLength)\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
        "\n",
        "    for i in range(len(scaling_factor)):\n",
        "        if abs(scaling_factor[i] - 1) < min_scale_sigma:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def DA_Scaling(X, sigma=0.3, min_scale_sigma=0.05):\n",
        "    scaling_factor = np.random.normal(\n",
        "        loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
        "    )  # shape=(1,3)\n",
        "    while is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
        "        scaling_factor = np.random.normal(\n",
        "            loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
        "        )\n",
        "    my_noise = np.matmul(np.ones((X.shape[0], 1)), scaling_factor)\n",
        "    X = X * my_noise\n",
        "    return X\n",
        "\n",
        "\n",
        "def scaling_uniform(X, scale_range=0.15, min_scale_diff=0.02):\n",
        "    low = 1 - scale_range\n",
        "    high = 1 + scale_range\n",
        "    scaling_factor = np.random.uniform(\n",
        "        low=low, high=high, size=(X.shape[1])\n",
        "    )  # shape=(3)\n",
        "    while is_scaling_factor_invalid(scaling_factor, min_scale_diff):\n",
        "        scaling_factor = np.random.uniform(\n",
        "            low=low, high=high, size=(X.shape[1])\n",
        "        )\n",
        "\n",
        "    for i in range(3):\n",
        "        X[:, i] = X[:, i] * scaling_factor[i]\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def scale(sample, choice, scale_range=0.5, min_scale_diff=0.15):\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = scaling_uniform(\n",
        "            sample, scale_range=scale_range, min_scale_diff=min_scale_diff\n",
        "        )\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def DistortTimesteps(X, sigma=0.2):\n",
        "    tt = GenerateRandomCurves(\n",
        "        X, sigma\n",
        "    )  # Regard these samples aroun 1 as time intervals\n",
        "    tt_cum = np.cumsum(tt, axis=0)  # Add intervals to make a cumulative graph\n",
        "    # Make the last value to have X.shape[0]\n",
        "    t_scale = [\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 0],\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 1],\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 2],\n",
        "    ]\n",
        "    tt_cum[:, 0] = tt_cum[:, 0] * t_scale[0]\n",
        "    tt_cum[:, 1] = tt_cum[:, 1] * t_scale[1]\n",
        "    tt_cum[:, 2] = tt_cum[:, 2] * t_scale[2]\n",
        "    return tt_cum\n",
        "\n",
        "\n",
        "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
        "    xx = (\n",
        "        np.ones((X.shape[1], 1))\n",
        "        * (np.arange(0, X.shape[0], (X.shape[0] - 1) / (knot + 1)))\n",
        "    ).transpose()\n",
        "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, X.shape[1]))\n",
        "    x_range = np.arange(X.shape[0])\n",
        "    cs_x = CubicSpline(xx[:, 0], yy[:, 0])\n",
        "    cs_y = CubicSpline(xx[:, 1], yy[:, 1])\n",
        "    cs_z = CubicSpline(xx[:, 2], yy[:, 2])\n",
        "    return np.array([cs_x(x_range), cs_y(x_range), cs_z(x_range)]).transpose()\n",
        "\n",
        "\n",
        "def DA_TimeWarp(X, sigma=0.2):\n",
        "    tt_new = DistortTimesteps(X, sigma)\n",
        "    X_new = np.zeros(X.shape)\n",
        "    x_range = np.arange(X.shape[0])\n",
        "    X_new[:, 0] = np.interp(x_range, tt_new[:, 0], X[:, 0])\n",
        "    X_new[:, 1] = np.interp(x_range, tt_new[:, 1], X[:, 1])\n",
        "    X_new[:, 2] = np.interp(x_range, tt_new[:, 2], X[:, 2])\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def time_warp(sample, choice, sigma=0.2):\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = DA_TimeWarp(sample, sigma=sigma)\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "def custom_virtual_data_generation_algorithm(data):\n",
        "\n",
        "     # Data augmentations\n",
        "    left = permute(data[:,:3].transpose(), 1)\n",
        "    right = time_warp(data[:,3:].transpose(), 1)\n",
        "\n",
        "    new_data = np.concatenate([left.transpose(), right.transpose()], axis=1)\n",
        "    return new_data\n",
        "\n",
        "def save_virtual_data(data, filename):\n",
        "\n",
        "  data.to_csv(os.path.join(virt_directory, filename+'.csv'), index=False)\n",
        "  return\n",
        "\n",
        "def custom_virtual_data_generation(train_data_dict):\n",
        "\n",
        "  for u, df in train_data_dict.items():\n",
        "    print('Generating virtual data from user %s.'% u)\n",
        "    # Extract sensor data and labels\n",
        "    raw_data = df[selected_columns[:6]].values\n",
        "    labels = df[selected_columns[-1]].values.reshape(-1,1)\n",
        "\n",
        "    tmp = custom_virtual_data_generation_algorithm(raw_data)\n",
        "\n",
        "    # Concatenate data with operation labels\n",
        "    virtual_data = np.concatenate([tmp, labels], axis=1)\n",
        "\n",
        "    # Convert np.array to dataframe\n",
        "    df = pd.DataFrame(virtual_data, columns=new_columns)\n",
        "\n",
        "    # Save data to /data/virtual/\n",
        "    save_virtual_data(df, u)\n",
        "    # df.to_csv(os.path.join(virt_directory, u+'.csv'), index=False)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create a new Python file and write the functions to it\n",
        "with open(rootdir+'/custom_functions.py', 'w') as file:\n",
        "    file.write(functions_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3PyqFMHNhhf0",
      "metadata": {
        "id": "3PyqFMHNhhf0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
