{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qingxinxia/OpenPackChallenge2025/blob/main/1.Data%20Augmentation%20Algorithm%20with%20HAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f251e07c11db0713",
      "metadata": {
        "collapsed": false,
        "id": "f251e07c11db0713"
      },
      "source": [
        "# Virtual Data Generation for Complex Industrial Activity Recognition\n",
        "\n",
        "\n",
        "\n",
        "This notebook has been designed for the 7th Factory Work Activity Recognition Challenge competition with the aim of Activity Recognition using REAL Accelerometer from OpenPack dataset and GENERATED Accelerometer created by participants.\n",
        "\n",
        "If you have any questions, please feel free to email qingxinxia@hkust-gz.edu.cn with the subject Factory Work Activity Recognition Challenge.\n",
        "\n",
        "About this dataset and challenge -> https://abc-research.github.io/challenge2025/\n",
        "\n",
        "This notebook was prepared by Qingxin Xia, Kei Tanigaki and Yoshimura Naoya.\n",
        "\n",
        "---\n",
        "\n",
        "# 工場作業行動認識のための仮想データ生成\n",
        "本ノートブックは、第7回工場作業行動認識チャレンジのために設計されました。本チャレンジでは、OpenPackデータセット作成に用いられた実際の加速度計と参加者によって生成された加速度センサデータを使用しています。\n",
        "\n",
        "ご質問がある場合は、件名を「工場作業行動認識チャレンジ」として、qingxinxia@hkust-gz.edu.cnまでお気軽にメールしてください。\n",
        "\n",
        "このデータセットとチャレンジについて -> https://abc-research.github.io/challenge2025/\n",
        "\n",
        "このノートブックはQingxin Xia, Kei TanigakiとYoshimura Naoyaによって準備されました。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kDKmsfvHW7SC",
      "metadata": {
        "id": "kDKmsfvHW7SC"
      },
      "source": [
        "# 1. Preparation\n",
        "\n",
        "---\n",
        "\n",
        "# 1. 準備する"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d843c988b9ef7b0",
      "metadata": {
        "collapsed": false,
        "id": "9d843c988b9ef7b0"
      },
      "source": [
        "## 1.1 Mount Drive\n",
        "\n",
        "This tutorial is made in Google Colab. So, first we need to connect the Google Drive to access the data. You can directly add folder path to access the data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Mount Drive\n",
        "\n",
        "本チュートリアルは Google Colab で作成されています。\n",
        "そのため、まず Google ドライブに接続してデータにアクセスする必要があります。\n",
        "フォルダーパスを追加することでドライブへのアクセスが可能になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943f2d92923662a1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-25T05:26:29.294878400Z",
          "start_time": "2024-11-25T05:26:29.291889Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "943f2d92923662a1",
        "outputId": "70510a44-d012-4c4f-a9c7-dae0c4c51d9c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adde54465537d1b3",
      "metadata": {
        "collapsed": false,
        "id": "adde54465537d1b3"
      },
      "source": [
        "## 1.2 Load Necessary Libraries and Prepare Environment\n",
        "\n",
        "---\n",
        "## 1.2 必要なライブラリをロードして環境を準備する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2574b92f4441c5d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:40:37.951189Z",
          "start_time": "2024-11-26T17:40:35.164127100Z"
        },
        "id": "a2574b92f4441c5d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from scipy.interpolate import CubicSpline  # for warping\n",
        "from einops import rearrange, repeat\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e4e4342fba656c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:35.657476100Z",
          "start_time": "2024-11-26T17:32:35.555296600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e4e4342fba656c",
        "outputId": "c42b4574-f614-4f64-dd01-d88b905c7632"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIUkKzP8Lxak",
      "metadata": {
        "id": "oIUkKzP8Lxak"
      },
      "source": [
        "## 1.3 Fixed Part\n",
        "\n",
        "Participants can change the splits portion and random seed to measure the robustness of their algorithms. When we evaluate the code, selected_columns and new_columns will not change. However, the split and random seed will change.\n",
        "\n",
        "\n",
        "---\n",
        "## 1.3 修正不可能な箇所\n",
        "\n",
        "参加者は、splitとrandom seedを変更して、アルゴリズムの堅牢性を測定できます。我々がコードを評価する際には、selected_columns と new_columns は変更されませんが、split と random seed は変更されます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2597443fadcb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2597443fadcb88",
        "outputId": "4f4ee93f-4aa0-43da-a219-ca8de75f65f9"
      },
      "outputs": [],
      "source": [
        "splits = [0.7, 0.1, 0.2]\n",
        "print('Randomly Split the real dataset into train, validation and test sets: %s'%str(splits))\n",
        "\n",
        "selected_columns = ['atr01/acc_x','atr01/acc_y','atr01/acc_z','atr02/acc_x','atr02/acc_y','atr02/acc_z','timestamp','operation']\n",
        "print('Select acceleration data of both wrists: %s'%selected_columns)\n",
        "\n",
        "new_columns = selected_columns[:6] + [selected_columns[-1]]\n",
        "print('Data for train, validation, and test: %s'%new_columns)\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    # Set seed for Python's random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Set seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # If using CUDA, set seed for GPU as well\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "\n",
        "# Set a fixed random seed\n",
        "seed_value = 2025\n",
        "set_random_seed(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2caace48e5668f",
      "metadata": {
        "collapsed": false,
        "id": "af2caace48e5668f"
      },
      "source": [
        "## 1.4 Create Folders\n",
        "\n",
        "Firstly, a folder is created to save OpenPack dataset: '/data/real/'.\n",
        "\n",
        "Participants can download OpenPack dataset by themselves. The data should be placed at: '/data/real/'.\n",
        "\n",
        "Another folder '/data/virtual/' will be created to save generated data.\n",
        " *(Note that, the size of this 'virtual' folder is limited to 500MB.)*\n",
        "\n",
        "---\n",
        "## 1.4 フォルダーの作成\n",
        "\n",
        "まず、OpenPack データセットを保存するためのフォルダー「/data/real/」が作成されます。\n",
        "\n",
        "参加者は OpenPack データセットを自分でダウンロードできます。データは「/data/real/」に配置する必要があります。\n",
        "\n",
        "生成されたデータを保存するために、別のフォルダー「/data/virtual/」が作成されます。\n",
        "\n",
        "*(この「virtual」フォルダーのサイズは500MBに制限されていることに注意してください。)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T15:23:45.250297800Z",
          "start_time": "2024-11-26T15:23:45.119289600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "08c306b8-be5b-45ba-debb-b68ae676f1fc"
      },
      "outputs": [],
      "source": [
        "realpath = r'/data/real'\n",
        "virtpath = r'/data/virtual'\n",
        "rootdir = r'/Virtual_Data_Generation/'  # replace with your project path\n",
        "real_directory = rootdir + realpath\n",
        "virt_directory = rootdir + virtpath\n",
        "\n",
        "# Create the directory\n",
        "os.makedirs(real_directory, exist_ok=True)\n",
        "print(f\"Directory '{realpath}' created successfully.\")\n",
        "\n",
        "# Create the directory\n",
        "os.makedirs(virt_directory, exist_ok=True)\n",
        "print(f\"Directory '{virtpath}' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7vHDrFrHILJi",
      "metadata": {
        "id": "7vHDrFrHILJi"
      },
      "source": [
        "## 1.5 Download data from Zenodo\n",
        "\n",
        "Participants can also use this code to download the OpenPack dataset.\n",
        "\n",
        "\n",
        "---\n",
        "## 1.5 Zenodo からデータをダウンロード\n",
        "\n",
        "参加者はこのコードを使用して OpenPack データセットをダウンロードすることができます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IaUSDHTOIQRj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaUSDHTOIQRj",
        "outputId": "6e4744cd-7acc-4547-f6b6-37d4466c3302"
      },
      "outputs": [],
      "source": [
        "# Construct the URL to the Zenodo API\n",
        "api_url = f\"https://zenodo.org/records/11059235\"\n",
        "\n",
        "# Send a request to the Zenodo API\n",
        "response = requests.get(api_url)\n",
        "response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "# # Parse the JSON response\n",
        "# data = response.json()\n",
        "\n",
        "# Extract the file information\n",
        "download_url = f\"https://zenodo.org/records/11059235/files/imu-with-operation-action-labels.zip?download=1\"\n",
        "\n",
        "# Download the file\n",
        "file_response = requests.get(download_url)\n",
        "file_response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "# Save the file\n",
        "file_path = os.path.join(real_directory, 'imu-with-operation-action-labels.zip')\n",
        "with open(file_path, 'wb') as f:\n",
        "    f.write(file_response.content)\n",
        "\n",
        "print(f\"Downloaded to {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a7941d4d94b239",
      "metadata": {
        "collapsed": false,
        "id": "30a7941d4d94b239"
      },
      "source": [
        "## 1.6 Unzip OpenPack dataset\n",
        "\n",
        "After placing the OpenPack dataset at the '/data/real/' folder, unzip the files and delete the zip files.\n",
        "\n",
        "---\n",
        "## 1.6 OpenPack データセットを解凍する\n",
        "\n",
        "OpenPack データセットを '/data/real/' フォルダに配置した後、ファイルを解凍し、zip ファイルを削除します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9a758fa4d9d166",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:40:11.083311800Z",
          "start_time": "2024-11-26T11:40:00.694264800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd9a758fa4d9d166",
        "outputId": "9e3b9abc-b188-464d-a63f-dfc1ee4d81c0"
      },
      "outputs": [],
      "source": [
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(real_directory):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(real_directory, filename)\n",
        "\n",
        "    # Check if the file is a zip file\n",
        "    if filename.endswith('.zip'):\n",
        "        # Open the zip file\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Extract all contents of the zip file into the directory\n",
        "            zip_ref.extractall(file_path[:-4])\n",
        "            print(f\"Extracted: {filename}\")\n",
        "\n",
        "        # Delete the zip file\n",
        "        os.remove(file_path)\n",
        "        print(f\"Deleted: {filename}\")\n",
        "\n",
        "print(\"All zip files have been processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5a8f3aead7a511",
      "metadata": {
        "collapsed": false,
        "id": "7a5a8f3aead7a511"
      },
      "source": [
        "# 2. Use Real Data to Generate Virtual Data\n",
        "\n",
        "Firstly, we randomly split the real data into a training set, validation set, and test set according to a specific ratio. Participants can then use the training set to generate virtual data. Finally, we train the network using both the training set and the virtual data, and we calculate the F1 score on the test set.\n",
        "\n",
        "The following code provides an example of generating virtual data from the training set. Note that: (1) The model structure is fixed and unchanged. (2) The split ratio for the training and test sets, as well as the random seed, will differ from the current settings, which requires the virtual data generation algorithm to be robust against varying data. (3) Participants are free to design data generation algorithms and save them to a specified path: '/data/virtual/', but the size of the virtual data is limited to 500MB.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. 実データを使用して仮想データを生成する\n",
        "\n",
        "まず、実データを特定の比率に従ってトレーニングセット、検証セット、テストセットにランダムに分割します。参加者はトレーニングセットを使用して仮想データを生成できます。最後に、トレーニングセットと仮想データの両方を使用してネットワークをトレーニングし、テストセットの F1 スコアを計算します。\n",
        "\n",
        "次のコードは、トレーニング セットから仮想データを生成する例を示しています。次の点に注意してください。\n",
        "\n",
        "(1) モデル構造は固定されており、変更することはできません。\n",
        "\n",
        "(2) トレーニングセットとテストセットの分割比率、およびランダムシードは現在の設定とは異なります。そのため、仮想データ生成アルゴリズムは変化するデータに対して堅牢である必要があります。\n",
        "\n",
        "(3) 参加者はデータ生成アルゴリズムを自由に設計し、指定されたパス「/data/virtual/」に保存できますが、仮想データのサイズは500MBに制限されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba365afde03f932",
      "metadata": {
        "collapsed": false,
        "id": "ba365afde03f932"
      },
      "source": [
        "## 2.1 Assign train users, validation users, and test users\n",
        "\n",
        "In the OpenPack dataset, U0xxx corresponds to user IDs, and S0xxx corresponds to different experiment settings.\n",
        "\n",
        "In this Challenge, we will only select training (real) and test data from S0100.\n",
        "\n",
        "---\n",
        "## 2.1 トレーニング ユーザー、検証ユーザー、テスト ユーザーを割り当てる\n",
        "\n",
        "OpenPack データセットでは、U0xxx はユーザー ID に対応し、S0xxx はさまざまな実験設定に対応します。\n",
        "\n",
        "このチャレンジでは、S0100 からトレーニング (実際の) データとテスト データのみを選択します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92d0517e92b247c",
      "metadata": {
        "collapsed": false,
        "id": "b92d0517e92b247c"
      },
      "source": [
        "## 2.2 Filter out un-used data\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 未使用のデータを除外する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c071d8483f0a0c41",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:58:21.525546600Z",
          "start_time": "2024-11-26T11:58:21.495067600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c071d8483f0a0c41",
        "outputId": "4d9f4914-5c6d-4fc4-fcba-2be1c9e53704"
      },
      "outputs": [],
      "source": [
        "user_paths = {}\n",
        "for root, dirs, files in os.walk(real_directory):\n",
        "    for file in files:\n",
        "        if file.endswith('S0100.csv'):\n",
        "            user_paths[file[:-10]] = os.path.join(root, file)\n",
        "        else:\n",
        "          os.remove(os.path.join(root, file))  # remove unused data\n",
        "for u, d in user_paths.items():\n",
        "    print('%s at: %s'% (u,d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e088a3db85d2fb82",
      "metadata": {
        "id": "e088a3db85d2fb82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fdd3fbfea9d0344",
      "metadata": {
        "collapsed": false,
        "id": "fdd3fbfea9d0344"
      },
      "source": [
        "## 2.3 Split users to train, validation, and test sets\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 ユーザーをトレーニング、検証、テストセットに分割する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9df5a7359851e8c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T12:16:52.484542700Z",
          "start_time": "2024-11-26T12:16:52.450090200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9df5a7359851e8c",
        "outputId": "346feb00-d758-4bec-b5da-86f78ceeede5"
      },
      "outputs": [],
      "source": [
        "userIDs = list(user_paths.keys())\n",
        "\n",
        "# Shuffle the list to ensure randomness\n",
        "random.shuffle(userIDs)\n",
        "\n",
        "# Calculate the split indices\n",
        "total_length = len(userIDs)\n",
        "train_size = int(total_length * splits[0])  # 70% of 10\n",
        "val_size = int(total_length * splits[1])  # 10% of 10\n",
        "test_size = total_length - train_size - val_size  # 20% of 10\n",
        "\n",
        "# Split the list according to the calculated sizes\n",
        "train_users = np.sort(userIDs[:train_size])      # First 70%\n",
        "val_users = np.sort(userIDs[train_size:train_size + val_size])  # Next 10%\n",
        "test_users = np.sort(userIDs[train_size + val_size:])  # Last 20%\n",
        "\n",
        "print('Training set: %s'%train_users)\n",
        "print('Validation set: %s'%val_users)\n",
        "print('Test set: %s'%test_users)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de07e6c2e80afb9",
      "metadata": {
        "collapsed": false,
        "id": "1de07e6c2e80afb9"
      },
      "source": [
        "## 2.4 Load data according to userIDs\n",
        "\n",
        "Load data of every user as dataframe.\n",
        "Use acceleration data of both wrists only;\n",
        "Use operation label.\n",
        "\n",
        "---\n",
        "## 2.4 ユーザーID に従ってデータをロードします\n",
        "\n",
        "すべてのユーザーのデータをデータフレームとしてロードします。\n",
        "両手首の加速度データのみを使用します。\n",
        "操作ラベルを使用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfbcc63914ccaf29",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T12:34:50.434498700Z",
          "start_time": "2024-11-26T12:34:45.441242400Z"
        },
        "id": "cfbcc63914ccaf29"
      },
      "outputs": [],
      "source": [
        "# selected_columns = ['atr01/acc_x','atr01/acc_y','atr01/acc_z','atr02/acc_x','atr02/acc_y','atr02/acc_z','timestamp','operation']\n",
        "train_data_dict = {}\n",
        "for u in train_users:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    train_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)\n",
        "\n",
        "val_data_dict = {}\n",
        "for u in val_users:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    val_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)\n",
        "\n",
        "test_data_dict = {}\n",
        "for u in test_users:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    test_data_dict[u] = pd.read_csv(user_paths[u], usecols=selected_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425d905a5d0e5459",
      "metadata": {
        "collapsed": false,
        "id": "425d905a5d0e5459"
      },
      "source": [
        "## 2.5 Show an example of data\n",
        "\n",
        "---\n",
        "## 2.5 データの例を示す\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650c28530895cbf4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T13:00:47.961638500Z",
          "start_time": "2024-11-26T13:00:47.116465Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "650c28530895cbf4",
        "outputId": "a396058f-72ca-49ef-dee7-f6113b5ed3b8"
      },
      "outputs": [],
      "source": [
        "df = train_data_dict[train_users[0]]\n",
        "\n",
        "n = 10  # only show n timestamps on fig\n",
        "# timezone_jst = datetime.timezone(datetime.timedelta(hours=9))\n",
        "# dates = [str(datetime.datetime.fromtimestamp(ts / 1000).replace(tzinfo=timezone_jst)) for ts in df['timestamp'].values]\n",
        "dates = df.timestamp.values\n",
        "# Select n equally spaced indices to show on the x-axis\n",
        "indices = np.linspace(0, len(dates) - 1, n, dtype=int)\n",
        "selected_dates = [dates[i] for i in indices]\n",
        "\n",
        "data = df[['atr01/acc_x','atr01/acc_y','atr01/acc_z']].values\n",
        "\n",
        "l = df['operation'].values\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(14, 6))\n",
        "# First subplot\n",
        "axs[0].plot(dates, data[:,0], label='x')\n",
        "axs[0].plot(dates, data[:,1], label='y')\n",
        "axs[0].plot(dates, data[:,2], label='z')\n",
        "axs[0].set_title('Raw data')\n",
        "axs[0].set_xlabel('timesteps')\n",
        "axs[0].set_ylabel('Value')\n",
        "axs[0].legend()\n",
        "# Set x-ticks for the current subplot\n",
        "# axs[0].set_xticks(selected_dates)\n",
        "# axs[0].set_xticklabels(selected_dates, rotation=60)  # Set labels and rotate\n",
        "# axs[0].set_xlim([dates[0], dates[-1]])  # Set x-axis limits\n",
        "# axs[0].grid()\n",
        "\n",
        "# Second subplot\n",
        "axs[1].plot(dates, l, label='label value')\n",
        "axs[1].set_title('Operation labels')\n",
        "axs[1].set_xlabel('timesteps')\n",
        "axs[1].set_ylabel('Label ID')\n",
        "axs[1].set_xticks(selected_dates)\n",
        "axs[1].set_xticklabels(selected_dates, rotation=30)  # Set labels and rotate\n",
        "axs[1].set_xlim([dates[0], dates[-1]])  # Set x-axis limits\n",
        "axs[1].legend()\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8117354536aa80c8",
      "metadata": {
        "id": "8117354536aa80c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "97fe49aa9ed13dcb",
      "metadata": {
        "collapsed": false,
        "id": "97fe49aa9ed13dcb"
      },
      "source": [
        "## 2.6 Example of Virtual Data Generation\n",
        "\n",
        "Here, we use data augmentations to generate virtual data. Participants are free to design virtual data generation algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.6 仮想データ生成の例\n",
        "\n",
        "ここでは、データ拡張を使用して仮想データを生成します。参加者は仮想データ生成アルゴリズムを自由に設計できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2967423ce58fab9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:15:04.268329100Z",
          "start_time": "2024-11-26T11:15:04.258405600Z"
        },
        "id": "2967423ce58fab9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code implements a list of transforms for tri-axial raw-accelerometry\n",
        "We assume that the input format is of size:\n",
        "3 x (epoch_len * sampling_frequency)\n",
        "\n",
        "Transformations included:\n",
        "1. jitter\n",
        "2. Channel shuffling: which axis is being switched\n",
        "3. Horizontal flip: binary\n",
        "4. Permutation: binary\n",
        "\n",
        "This script is mostly based off from\n",
        "https://github.com/terryum/Data-Augmentation-For-Wearable-Sensor-Data/blob/master/Example_DataAugmentation_TimeseriesData.py\n",
        "\"\"\"\n",
        "\n",
        "def switch_axis(sample, choice):\n",
        "    \"\"\"\n",
        "    Randomly switch the three axises for the raw files\n",
        "\n",
        "    Args:\n",
        "        sample (numpy array): 3 * FEATURE_SIZE\n",
        "        choice (int): 0-6 for direction selection\n",
        "    \"\"\"\n",
        "    x = sample[0, :]\n",
        "    y = sample[1, :]\n",
        "    z = sample[2, :]\n",
        "\n",
        "    if choice == 0:\n",
        "        return sample\n",
        "    elif choice == 1:\n",
        "        sample = np.stack([x, y, z], axis=0)\n",
        "    elif choice == 2:\n",
        "        sample = np.stack([x, z, y], axis=0)\n",
        "    elif choice == 3:\n",
        "        sample = np.stack([y, x, z], axis=0)\n",
        "    elif choice == 4:\n",
        "        sample = np.stack([y, z, x], axis=0)\n",
        "    elif choice == 5:\n",
        "        sample = np.stack([z, x, y], axis=0)\n",
        "    elif choice == 6:\n",
        "        sample = np.stack([z, y, x], axis=0)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def flip(sample, choice):\n",
        "    \"\"\"\n",
        "    Flip over the actigram on the temporal scale\n",
        "\n",
        "    Args:\n",
        "        sample (numpy array): 3 * FEATURE_SIZE\n",
        "        choice (int): 0-1 binary\n",
        "    \"\"\"\n",
        "    if choice == 1:\n",
        "        sample = np.flip(sample, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
        "    X_new = np.zeros(X.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile is True:\n",
        "        segs = np.zeros(nPerm + 1, dtype=int)\n",
        "        segs[1:-1] = np.sort(\n",
        "            np.random.randint(\n",
        "                minSegLength, X.shape[0] - minSegLength, nPerm - 1\n",
        "            )\n",
        "        )\n",
        "        segs[-1] = X.shape[0]\n",
        "        if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        x_temp = X[segs[idx[ii]] : segs[idx[ii] + 1], :]\n",
        "        X_new[pp : pp + len(x_temp), :] = x_temp\n",
        "        pp += len(x_temp)\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def permute(sample, choice, nPerm=4, minSegLength=10):\n",
        "    \"\"\"\n",
        "    Distort an epoch by dividing up the sample into several segments and\n",
        "    then permute them\n",
        "\n",
        "    Args:\n",
        "        sample (numpy array): 3 * FEATURE_SIZE\n",
        "        choice (int): 0-1 binary\n",
        "    \"\"\"\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = DA_Permutation(sample, nPerm=nPerm, minSegLength=minSegLength)\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
        "    \"\"\"\n",
        "    Ensure each of the abs values of the scaling\n",
        "    factors are greater than the min\n",
        "    \"\"\"\n",
        "    for i in range(len(scaling_factor)):\n",
        "        if abs(scaling_factor[i] - 1) < min_scale_sigma:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def DA_Scaling(X, sigma=0.3, min_scale_sigma=0.05):\n",
        "    scaling_factor = np.random.normal(\n",
        "        loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
        "    )  # shape=(1,3)\n",
        "    while is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
        "        scaling_factor = np.random.normal(\n",
        "            loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
        "        )\n",
        "    my_noise = np.matmul(np.ones((X.shape[0], 1)), scaling_factor)\n",
        "    X = X * my_noise\n",
        "    return X\n",
        "\n",
        "\n",
        "def scaling_uniform(X, scale_range=0.15, min_scale_diff=0.02):\n",
        "    low = 1 - scale_range\n",
        "    high = 1 + scale_range\n",
        "    scaling_factor = np.random.uniform(\n",
        "        low=low, high=high, size=(X.shape[1])\n",
        "    )  # shape=(3)\n",
        "    while is_scaling_factor_invalid(scaling_factor, min_scale_diff):\n",
        "        scaling_factor = np.random.uniform(\n",
        "            low=low, high=high, size=(X.shape[1])\n",
        "        )\n",
        "\n",
        "    for i in range(3):\n",
        "        X[:, i] = X[:, i] * scaling_factor[i]\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def scale(sample, choice, scale_range=0.5, min_scale_diff=0.15):\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = scaling_uniform(\n",
        "            sample, scale_range=scale_range, min_scale_diff=min_scale_diff\n",
        "        )\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def DistortTimesteps(X, sigma=0.2):\n",
        "    tt = GenerateRandomCurves(\n",
        "        X, sigma\n",
        "    )  # Regard these samples aroun 1 as time intervals\n",
        "    tt_cum = np.cumsum(tt, axis=0)  # Add intervals to make a cumulative graph\n",
        "    # Make the last value to have X.shape[0]\n",
        "    t_scale = [\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 0],\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 1],\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 2],\n",
        "    ]\n",
        "    tt_cum[:, 0] = tt_cum[:, 0] * t_scale[0]\n",
        "    tt_cum[:, 1] = tt_cum[:, 1] * t_scale[1]\n",
        "    tt_cum[:, 2] = tt_cum[:, 2] * t_scale[2]\n",
        "    return tt_cum\n",
        "\n",
        "\n",
        "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
        "    xx = (\n",
        "        np.ones((X.shape[1], 1))\n",
        "        * (np.arange(0, X.shape[0], (X.shape[0] - 1) / (knot + 1)))\n",
        "    ).transpose()\n",
        "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, X.shape[1]))\n",
        "    x_range = np.arange(X.shape[0])\n",
        "    cs_x = CubicSpline(xx[:, 0], yy[:, 0])\n",
        "    cs_y = CubicSpline(xx[:, 1], yy[:, 1])\n",
        "    cs_z = CubicSpline(xx[:, 2], yy[:, 2])\n",
        "    return np.array([cs_x(x_range), cs_y(x_range), cs_z(x_range)]).transpose()\n",
        "\n",
        "\n",
        "def DA_TimeWarp(X, sigma=0.2):\n",
        "    tt_new = DistortTimesteps(X, sigma)\n",
        "    X_new = np.zeros(X.shape)\n",
        "    x_range = np.arange(X.shape[0])\n",
        "    X_new[:, 0] = np.interp(x_range, tt_new[:, 0], X[:, 0])\n",
        "    X_new[:, 1] = np.interp(x_range, tt_new[:, 1], X[:, 1])\n",
        "    X_new[:, 2] = np.interp(x_range, tt_new[:, 2], X[:, 2])\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def time_warp(sample, choice, sigma=0.2):\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = DA_TimeWarp(sample, sigma=sigma)\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860007434690971",
      "metadata": {
        "collapsed": false,
        "id": "860007434690971"
      },
      "source": [
        "## 2.7 Visualization of an example of data augmentation\n",
        "\n",
        "---\n",
        "\n",
        "## 2.7 データ拡張の例の視覚化図"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b82dd20fa515a8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T11:36:38.907602400Z",
          "start_time": "2024-11-26T11:36:38.346568200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "13b82dd20fa515a8",
        "outputId": "d5eb794b-47ba-4538-dab5-608157cca93f"
      },
      "outputs": [],
      "source": [
        "random_array_normal = np.random.randn(3, 100)\n",
        "warped_data = time_warp(random_array_normal, 1, sigma=0.2)\n",
        "x = np.linspace(0, 100, 100)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# First subplot\n",
        "axs[0].plot(x, random_array_normal[0,:], label='x')\n",
        "axs[0].plot(x, random_array_normal[1,:], label='y')\n",
        "axs[0].plot(x, random_array_normal[2,:], label='z')\n",
        "axs[0].set_title('Raw data')\n",
        "axs[0].set_xlabel('timesteps')\n",
        "axs[0].set_ylabel('Value')\n",
        "axs[0].legend()\n",
        "\n",
        "# Second subplot\n",
        "axs[1].plot(warped_data[0,:], label='x')\n",
        "axs[1].plot(warped_data[1,:], label='y')\n",
        "axs[1].plot(warped_data[2,:], label='z')\n",
        "axs[1].set_title('Generated data')\n",
        "axs[1].set_xlabel('timesteps')\n",
        "axs[1].set_ylabel('Value')\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "908ae10f54d3cfd",
      "metadata": {
        "collapsed": false,
        "id": "908ae10f54d3cfd"
      },
      "source": [
        "## 2.8 Implement data augmentation with training data and save them to folder\n",
        "\n",
        "Please save the generated data as csv file in '/data/virtual/'.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.8 トレーニング データを使用してデータ拡張を実装し、フォルダーに保存します\n",
        "\n",
        "生成されたデータを '/data/virtual/' に csv ファイルとして保存してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a44a5f3977b74",
      "metadata": {
        "id": "25a44a5f3977b74"
      },
      "outputs": [],
      "source": [
        "def custom_virtual_data_generation_algorithm(data):\n",
        "    '''\n",
        "    Please modify the code and submit this function and its relative functions.\n",
        "    :param data: numpy array, shape is (data length, dim=6)\n",
        "    :return: numpy array, shape is (data length, dim=6)\n",
        "    '''\n",
        "     # Data augmentations\n",
        "    left = permute(data[:,:3].transpose(), 1)\n",
        "    right = time_warp(data[:,3:].transpose(), 1)\n",
        "\n",
        "    new_data = np.concatenate([left.transpose(), right.transpose()], axis=1)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zjD_UzRNYLAt",
      "metadata": {
        "id": "zjD_UzRNYLAt"
      },
      "outputs": [],
      "source": [
        "def save_virtual_data(data, filename):\n",
        "  '''\n",
        "  Participants can use this function to save csv data to /data/virtual/\n",
        "  :param data: dataframe type, shape is (data length, dim=7), columns = new_columns = selected_columns[:6] + [selected_columns[-1]]\n",
        "  :return:\n",
        "  '''\n",
        "\n",
        "  data.to_csv(os.path.join(virt_directory, filename+'.csv'), index=False)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNP6ZYGyfFHO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNP6ZYGyfFHO",
        "outputId": "89c4b79c-929d-4293-c809-3c94460b586f"
      },
      "outputs": [],
      "source": [
        "def custom_virtual_data_generation(train_data_dict):\n",
        "  '''\n",
        "  This function aims to generate virtual and from train_data_dict, and save the data to virt_directory.\n",
        "  Participants could not change the input and output of this function.\n",
        "  Participants could modify the code inside this function.\n",
        "  During the code submission, participants need to submit this function and its relavant functions, such as custom_virtual_data_generation_algorithm.\n",
        "  '''\n",
        "  for u, df in train_data_dict.items():\n",
        "    print('Generating virtual data from user %s.'% u)\n",
        "    # Extract sensor data and labels\n",
        "    raw_data = df[selected_columns[:6]].values\n",
        "    labels = df[selected_columns[-1]].values.reshape(-1,1)\n",
        "\n",
        "    tmp = custom_virtual_data_generation_algorithm(raw_data)\n",
        "\n",
        "    # Concatenate data with operation labels\n",
        "    virtual_data = np.concatenate([tmp, labels], axis=1)\n",
        "\n",
        "    # Convert np.array to dataframe\n",
        "    df = pd.DataFrame(virtual_data, columns=new_columns)\n",
        "\n",
        "    # Save data to /data/virtual/\n",
        "    save_virtual_data(df, u)\n",
        "    # df.to_csv(os.path.join(virt_directory, u+'.csv'), index=False)\n",
        "\n",
        "custom_virtual_data_generation(train_data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb41e3073faa5f67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "cb41e3073faa5f67",
        "outputId": "9c35e29d-b1d5-476c-b874-a96dc72b11b6"
      },
      "outputs": [],
      "source": [
        "# Example of virtual data structure\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vVnsE1SKcMJg",
      "metadata": {
        "id": "vVnsE1SKcMJg"
      },
      "source": [
        "##2.9 Check the size of virtual data\n",
        "\n",
        "Use this code to ensure the size of virtual data is not exceeded to 500MB.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "##2.9 仮想データのサイズを確認する\n",
        "\n",
        "このコードを使用して、仮想データのサイズが 500MB を超えていないことを確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WDy12Am-ceS-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDy12Am-ceS-",
        "outputId": "c5d1e92b-2d1a-412e-8c95-ae73a761c0fc"
      },
      "outputs": [],
      "source": [
        "def get_folder_size(folder_path):\n",
        "    total_size = 0\n",
        "    # Walk through all files and subdirectories in the folder\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            # Get the full file path\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            # Add the size of each file to the total size\n",
        "            total_size += os.path.getsize(file_path)\n",
        "    return total_size\n",
        "\n",
        "# Example usage\n",
        "size = get_folder_size(virt_directory)\n",
        "print(f\"The total size of the folder is: {size} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c18c2a90cf62034",
      "metadata": {
        "collapsed": false,
        "id": "2c18c2a90cf62034"
      },
      "source": [
        "# 3. Use the Generated Data to Improve HAR Model Performance\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 3. 生成されたデータを使用してHARモデルのパフォーマンスを向上させる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6356dfab9eb38848",
      "metadata": {
        "collapsed": false,
        "id": "6356dfab9eb38848"
      },
      "source": [
        "## 3.1 Read data and labels from both real and virtual folders\n",
        "\n",
        "\n",
        "---\n",
        "## 3.1 実フォルダと仮想フォルダの両方からデータとラベルを読み取る\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52a01881aa3e839",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:06:15.505458200Z",
          "start_time": "2024-11-26T17:06:15.440409700Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52a01881aa3e839",
        "outputId": "bd831f67-89d0-4c0e-8ae1-18682c18372c"
      },
      "outputs": [],
      "source": [
        "# find csv files in 'data/virtual'\n",
        "virt_paths = []\n",
        "for root, dirs, files in os.walk(virt_directory):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            virt_paths.append(os.path.join(root, file))\n",
        "print('Virtual csv file paths are as shown follows:')\n",
        "virt_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317fd3932804be3e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:06:31.374298Z",
          "start_time": "2024-11-26T17:06:29.371508900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "317fd3932804be3e",
        "outputId": "935fa838-384b-4002-f510-7e4fdfcfc6de"
      },
      "outputs": [],
      "source": [
        "# real and virtual training data\n",
        "\n",
        "## real data\n",
        "train_data = []\n",
        "for u, data in train_data_dict.items():\n",
        "    train_data.append(data[new_columns].values)\n",
        "    # print(data[new_columns].values.shape)\n",
        "\n",
        "## virtual data\n",
        "for p in virt_paths:\n",
        "    # Load the CSV file with only the selected columns\n",
        "    data = pd.read_csv(p, usecols=new_columns)\n",
        "    train_data.append(data.values)\n",
        "\n",
        "train_data = np.concatenate(train_data, axis=0)\n",
        "print('Shape of train data is %s'%str(train_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad48585ae2ea52d6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:07:26.403366900Z",
          "start_time": "2024-11-26T17:07:26.246585200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad48585ae2ea52d6",
        "outputId": "4761d4d7-63dd-40e4-83e8-199880840494"
      },
      "outputs": [],
      "source": [
        "# validatation and test data\n",
        "val_data = []\n",
        "for u, data in val_data_dict.items():\n",
        "    val_data.append(data[new_columns].values)\n",
        "\n",
        "test_data = []\n",
        "for u, data in test_data_dict.items():\n",
        "    test_data.append(data[new_columns].values)\n",
        "\n",
        "val_data = np.concatenate(val_data, axis=0)\n",
        "test_data = np.concatenate(test_data, axis=0)\n",
        "\n",
        "print('Shape of validation data is %s'%str(val_data.shape))\n",
        "print('Shape of test data is %s'%str(test_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5853fe098d9073b0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:09:38.449495900Z",
          "start_time": "2024-11-26T17:09:37.884790900Z"
        },
        "id": "5853fe098d9073b0"
      },
      "outputs": [],
      "source": [
        "# convert operation ID to labels (from 0 to n)\n",
        "labels = np.unique(train_data[:, -1])\n",
        "label_dict = dict(zip(labels, np.arange(len(labels))))\n",
        "train_data[:,-1] = np.array([label_dict[i] for i in train_data[:,-1]])\n",
        "val_data[:,-1] =  np.array([label_dict[i] for i in val_data[:,-1]])\n",
        "test_data[:,-1] =  np.array([label_dict[i] for i in test_data[:,-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f7ecdf96bd4fff",
      "metadata": {
        "id": "a4f7ecdf96bd4fff"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9693fd76fa677383",
      "metadata": {
        "collapsed": false,
        "id": "9693fd76fa677383"
      },
      "source": [
        "## 3.2 Prepare Dataloader\n",
        "\n",
        "---\n",
        "\n",
        "## 3.2 データローダーの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5941002ccfa02913",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:10:20.929443300Z",
          "start_time": "2024-11-26T17:10:20.898518300Z"
        },
        "id": "5941002ccfa02913"
      },
      "outputs": [],
      "source": [
        "class data_loader_OpenPack(Dataset):\n",
        "    def __init__(self, samples, labels, device='cpu'):\n",
        "        self.samples = torch.tensor(samples).to(device)  # check data type\n",
        "        self.labels = torch.tensor(labels)  # check data type\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.labels[index]\n",
        "        sample = self.samples[index]\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def sliding_window(datanp, len_sw, step):\n",
        "    '''\n",
        "    :param datanp: shape=(data length, dim) raw sensor data and the labels. The last column is the label column.\n",
        "    :param len_sw: length of the segmented sensor data\n",
        "    :param step: overlapping length of the segmented data\n",
        "    :return: shape=(N, len_sw, dim) batch of sensor data segment.\n",
        "    '''\n",
        "\n",
        "    # generate batch of data by overlapping the training set\n",
        "    data_batch = []\n",
        "    for idx in range(0, datanp.shape[0] - len_sw - step, step):\n",
        "        data_batch.append(datanp[idx: idx + len_sw, :])\n",
        "    data_batch.append(datanp[-1 - len_sw: -1, :])  # last batch\n",
        "    xlist = np.stack(data_batch, axis=0)  # [B, data length, dim]\n",
        "\n",
        "    return xlist\n",
        "\n",
        "def generate_dataloader(data, len_sw, step, if_shuffle=True):\n",
        "    tmp_b = sliding_window(data, len_sw, step)\n",
        "    data_b = tmp_b[:, :, :-1]\n",
        "    label_b = tmp_b[:, :, -1]\n",
        "    data_set_r = data_loader_OpenPack(data_b, label_b, device=device)\n",
        "    data_loader = DataLoader(data_set_r, batch_size=batch_size,\n",
        "                              shuffle=if_shuffle, drop_last=False)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d6e62da61c17d9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:10:24.113448400Z",
          "start_time": "2024-11-26T17:10:23.094960500Z"
        },
        "id": "d7d6e62da61c17d9"
      },
      "outputs": [],
      "source": [
        "len_sw = 300\n",
        "step = 150\n",
        "batch_size = 512\n",
        "\n",
        "train_loader = generate_dataloader(train_data, len_sw, step, if_shuffle=True)\n",
        "val_loader = generate_dataloader(val_data, len_sw, step, if_shuffle=False)\n",
        "test_loader = generate_dataloader(test_data, len_sw, step, if_shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be50fc0f4fdbbb16",
      "metadata": {
        "collapsed": false,
        "id": "be50fc0f4fdbbb16"
      },
      "source": [
        "## 3.3 Prepare Model\n",
        "\n",
        "Reference:\n",
        "https://github.com/Tian0426/CL-HAR\n",
        "\n",
        "---\n",
        "## 3.3 モデルの準備\n",
        "\n",
        "参照:\n",
        "https://github.com/Tian0426/CL-HAR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c655b4c595520ad2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:31:54.694552300Z",
          "start_time": "2024-11-26T17:31:54.672758900Z"
        },
        "id": "c655b4c595520ad2"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value=True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        self.attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', self.attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class Transformer_block(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                (PreNorm(dim, Attention(dim, heads=heads, dropout=dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Seq_Transformer(nn.Module):\n",
        "    def __init__(self, n_channel, len_sw, n_classes, dim=128, depth=4, heads=4, mlp_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_to_embedding = nn.Linear(n_channel, dim)\n",
        "        self.c_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.position = PositionalEncoding(d_model=dim, max_len=len_sw)\n",
        "        self.transformer = Transformer_block(dim, depth, heads, mlp_dim, dropout)\n",
        "        self.to_c_token = nn.Identity()\n",
        "        self.classifier = nn.Linear(dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, forward_seq):\n",
        "        x = self.patch_to_embedding(forward_seq)\n",
        "        x = self.position(x)\n",
        "        b, n, _ = x.shape\n",
        "        c_tokens = repeat(self.c_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((c_tokens, x), dim=1)\n",
        "        x = self.transformer(x)\n",
        "        c_t = self.to_c_token(x[:, 0])\n",
        "        return c_t\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_channels=6, len_sw=300, n_classes=11, dim=128, depth=4, heads=4, mlp_dim=64, dropout=0.3):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.out_dim = dim\n",
        "        self.transformer = Seq_Transformer(n_channel=n_channels, len_sw=len_sw, n_classes=n_classes, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim, dropout=dropout)\n",
        "        self.classifier = nn.Linear(dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer(x)\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "        # return out, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655130025ce06d58",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:47.180838500Z",
          "start_time": "2024-11-26T17:32:47.100693300Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655130025ce06d58",
        "outputId": "a5dc62f5-bf5e-4841-f6e4-5d016a7a75e3"
      },
      "outputs": [],
      "source": [
        "model = Transformer()\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7b7fef2f95d4f0",
      "metadata": {
        "collapsed": false,
        "id": "1c7b7fef2f95d4f0"
      },
      "source": [
        "## 3.4 Training and test\n",
        "\n",
        "Reference:\n",
        "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/main.py\n",
        "\n",
        "---\n",
        "## 3.4 トレーニングとテスト\n",
        "\n",
        "参考:\n",
        "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbc8df0df53ab2d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:49.432241200Z",
          "start_time": "2024-11-26T17:32:49.394337400Z"
        },
        "id": "fcbc8df0df53ab2d"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                if self.verbose:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                return True\n",
        "        return False\n",
        "early_stopping = EarlyStopping()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PZNkw85JqQ4O",
      "metadata": {
        "id": "PZNkw85JqQ4O"
      },
      "outputs": [],
      "source": [
        "def vote_labels(label):\n",
        "    # Iterate over each sample in the batch\n",
        "    votel = []\n",
        "    for i in range(label.size(0)):\n",
        "        # Get unique labels and their counts\n",
        "        unique_labels, counts = label[i].unique(return_counts=True)\n",
        "\n",
        "        # Find the index of the maximum count\n",
        "        max_count_index = counts.argmax()\n",
        "\n",
        "        # Get the label corresponding to that maximum count\n",
        "        mode_label = unique_labels[max_count_index]\n",
        "\n",
        "        # Append the mode to the result list\n",
        "        votel.append(mode_label)\n",
        "\n",
        "    # Convert the result list to a tensor and reshape to (batch, 1)\n",
        "    vote_label = torch.tensor(votel, dtype=torch.long).view(-1)\n",
        "    return vote_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97897e6f2ad9e6cd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:55.039601400Z",
          "start_time": "2024-11-26T17:32:54.946321600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "97897e6f2ad9e6cd",
        "outputId": "bdd74dd4-2414-4b83-fbb4-4315c2a34f3d"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(\n",
        "            model.parameters(), lr=learning_rate, amsgrad=True\n",
        "        )\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18f514279c93576",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:32:56.222566600Z",
          "start_time": "2024-11-26T17:32:56.048601700Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a18f514279c93576",
        "outputId": "e7c00691-005c-4281-ae04-933fac8e6689"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = [], []\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train_loss, val_loss = [], []\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    true_labels, pred_labels = [], []\n",
        "    for i, (sample, label) in enumerate(train_loader):\n",
        "        sample = sample.to(device=device, dtype=torch.float)\n",
        "        label = label.to(device=device, dtype=torch.long)\n",
        "        vote_label = vote_labels(label)\n",
        "        vote_label = vote_label.to(device)\n",
        "        output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
        "        loss = criterion(output, vote_label)\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        true_labels.append(vote_label.detach().cpu().numpy())\n",
        "        pred_labels.append(output.detach().cpu().numpy())\n",
        "\n",
        "    train_losses.append(np.average(train_loss))\n",
        "    # Calculate F1 scores\n",
        "    y_true = np.concatenate(true_labels, axis=0)\n",
        "    y_prob = np.concatenate(pred_labels, axis=0)\n",
        "\n",
        "    # Get the predicted class labels (argmax along the class dimension)\n",
        "    y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
        "\n",
        "    # Calculate F1 score (macro F1 score)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f'F1 Score of training set: {f1:.4f}')\n",
        "\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        true_labels, pred_labels = [], []\n",
        "        for i, (sample, label) in enumerate(val_loader):\n",
        "            sample = sample.to(device=device, dtype=torch.float)\n",
        "            label = label.to(device=device, dtype=torch.long)\n",
        "            vote_label = vote_labels(label)\n",
        "            vote_label = vote_label.to(device)\n",
        "            output = model(sample)\n",
        "            loss = criterion(output, vote_label)\n",
        "            val_loss.append(loss.item())\n",
        "            true_labels.append(vote_label.detach().cpu().numpy())\n",
        "            pred_labels.append(output.detach().cpu().numpy())\n",
        "        val_losses.append(np.average(val_loss))\n",
        "\n",
        "        # Calculate F1 scores\n",
        "        y_true = np.concatenate(true_labels, axis=0)\n",
        "        y_prob = np.concatenate(pred_labels, axis=0)\n",
        "\n",
        "        # Get the predicted class labels (argmax along the class dimension)\n",
        "        y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
        "\n",
        "        # Calculate F1 score (macro F1 score)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        print(f'F1 Score of validation set: {f1:.4f}')\n",
        "\n",
        "        # Check early stopping\n",
        "        if early_stopping(np.average(val_losses)):\n",
        "            print(\"Stopping at epoch %s.\" % str(epoch))\n",
        "            break\n",
        "    scheduler.step(np.average(val_loss))\n",
        "    # Print the current learning rate\n",
        "    current_lr = scheduler.get_last_lr()[0]  # Get the current learning rate\n",
        "    print(f'Epoch {epoch + 1}, Learning Rate: {current_lr}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MuGFwbEeaNN1",
      "metadata": {
        "id": "MuGFwbEeaNN1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(val_losses, label='valid loss')\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6e208b8a744644",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-26T17:13:54.424336Z",
          "start_time": "2024-11-26T17:13:54.391946400Z"
        },
        "id": "9b6e208b8a744644"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    true_labels, pred_labels = [], []\n",
        "    for i, (sample, label) in enumerate(test_loader):\n",
        "        sample = sample.to(device=device, dtype=torch.float)\n",
        "        label = label.to(device=device, dtype=torch.long)\n",
        "        vote_label = vote_labels(label)\n",
        "        # vote_label = vote_label.to(device)\n",
        "        output = model(sample)  # x_encoded.shape=batch512,outchannel128,len13\n",
        "\n",
        "        true_labels.append(vote_label.numpy())\n",
        "        pred_labels.append(output.detach().cpu().numpy())\n",
        "\n",
        "    # Calculate F1 scores\n",
        "    y_true = np.concatenate(true_labels, axis=0)\n",
        "    y_prob = np.concatenate(pred_labels, axis=0)\n",
        "\n",
        "    # Get the predicted class labels (argmax along the class dimension)\n",
        "    y_pred = np.argmax(y_prob, axis=1)  # output Shape: (batch_size, time_steps)\n",
        "\n",
        "    # Calculate F1 score (macro F1 score)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f'F1 Score of test set: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8JL7F0XeeK5",
      "metadata": {
        "id": "V8JL7F0XeeK5"
      },
      "source": [
        "# 4. Submission\n",
        "\n",
        "Participants only need to submit the code related to the virtual data generation.\n",
        "During the evaluation of the challenge, we will call \"custom_virtual_data_generation\" function to generate virtual data. So, please ensure that all relevant codes are included in the .py file.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 4. 提出\n",
        "\n",
        "参加者は仮想データ生成に関連するコードのみを提出する必要があります。\n",
        "チャレンジの評価中に、仮想データを生成するために「custom_virtual_data_generation」関数を呼び出します。ここでは、関連するすべてのコードが .py ファイルに含まれていることを確認してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8YotuUraesYD",
      "metadata": {
        "id": "8YotuUraesYD"
      },
      "outputs": [],
      "source": [
        "# Define the functions you want to save\n",
        "functions_code = \"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def switch_axis(sample, choice):\n",
        "\n",
        "    x = sample[0, :]\n",
        "    y = sample[1, :]\n",
        "    z = sample[2, :]\n",
        "\n",
        "    if choice == 0:\n",
        "        return sample\n",
        "    elif choice == 1:\n",
        "        sample = np.stack([x, y, z], axis=0)\n",
        "    elif choice == 2:\n",
        "        sample = np.stack([x, z, y], axis=0)\n",
        "    elif choice == 3:\n",
        "        sample = np.stack([y, x, z], axis=0)\n",
        "    elif choice == 4:\n",
        "        sample = np.stack([y, z, x], axis=0)\n",
        "    elif choice == 5:\n",
        "        sample = np.stack([z, x, y], axis=0)\n",
        "    elif choice == 6:\n",
        "        sample = np.stack([z, y, x], axis=0)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def flip(sample, choice):\n",
        "\n",
        "    if choice == 1:\n",
        "        sample = np.flip(sample, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
        "    X_new = np.zeros(X.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile is True:\n",
        "        segs = np.zeros(nPerm + 1, dtype=int)\n",
        "        segs[1:-1] = np.sort(\n",
        "            np.random.randint(\n",
        "                minSegLength, X.shape[0] - minSegLength, nPerm - 1\n",
        "            )\n",
        "        )\n",
        "        segs[-1] = X.shape[0]\n",
        "        if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        x_temp = X[segs[idx[ii]] : segs[idx[ii] + 1], :]\n",
        "        X_new[pp : pp + len(x_temp), :] = x_temp\n",
        "        pp += len(x_temp)\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def permute(sample, choice, nPerm=4, minSegLength=10):\n",
        "\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = DA_Permutation(sample, nPerm=nPerm, minSegLength=minSegLength)\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
        "\n",
        "    for i in range(len(scaling_factor)):\n",
        "        if abs(scaling_factor[i] - 1) < min_scale_sigma:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def DA_Scaling(X, sigma=0.3, min_scale_sigma=0.05):\n",
        "    scaling_factor = np.random.normal(\n",
        "        loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
        "    )  # shape=(1,3)\n",
        "    while is_scaling_factor_invalid(scaling_factor, min_scale_sigma):\n",
        "        scaling_factor = np.random.normal(\n",
        "            loc=1.0, scale=sigma, size=(1, X.shape[1])\n",
        "        )\n",
        "    my_noise = np.matmul(np.ones((X.shape[0], 1)), scaling_factor)\n",
        "    X = X * my_noise\n",
        "    return X\n",
        "\n",
        "\n",
        "def scaling_uniform(X, scale_range=0.15, min_scale_diff=0.02):\n",
        "    low = 1 - scale_range\n",
        "    high = 1 + scale_range\n",
        "    scaling_factor = np.random.uniform(\n",
        "        low=low, high=high, size=(X.shape[1])\n",
        "    )  # shape=(3)\n",
        "    while is_scaling_factor_invalid(scaling_factor, min_scale_diff):\n",
        "        scaling_factor = np.random.uniform(\n",
        "            low=low, high=high, size=(X.shape[1])\n",
        "        )\n",
        "\n",
        "    for i in range(3):\n",
        "        X[:, i] = X[:, i] * scaling_factor[i]\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def scale(sample, choice, scale_range=0.5, min_scale_diff=0.15):\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = scaling_uniform(\n",
        "            sample, scale_range=scale_range, min_scale_diff=min_scale_diff\n",
        "        )\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def DistortTimesteps(X, sigma=0.2):\n",
        "    tt = GenerateRandomCurves(\n",
        "        X, sigma\n",
        "    )  # Regard these samples aroun 1 as time intervals\n",
        "    tt_cum = np.cumsum(tt, axis=0)  # Add intervals to make a cumulative graph\n",
        "    # Make the last value to have X.shape[0]\n",
        "    t_scale = [\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 0],\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 1],\n",
        "        (X.shape[0] - 1) / tt_cum[-1, 2],\n",
        "    ]\n",
        "    tt_cum[:, 0] = tt_cum[:, 0] * t_scale[0]\n",
        "    tt_cum[:, 1] = tt_cum[:, 1] * t_scale[1]\n",
        "    tt_cum[:, 2] = tt_cum[:, 2] * t_scale[2]\n",
        "    return tt_cum\n",
        "\n",
        "\n",
        "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
        "    xx = (\n",
        "        np.ones((X.shape[1], 1))\n",
        "        * (np.arange(0, X.shape[0], (X.shape[0] - 1) / (knot + 1)))\n",
        "    ).transpose()\n",
        "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, X.shape[1]))\n",
        "    x_range = np.arange(X.shape[0])\n",
        "    cs_x = CubicSpline(xx[:, 0], yy[:, 0])\n",
        "    cs_y = CubicSpline(xx[:, 1], yy[:, 1])\n",
        "    cs_z = CubicSpline(xx[:, 2], yy[:, 2])\n",
        "    return np.array([cs_x(x_range), cs_y(x_range), cs_z(x_range)]).transpose()\n",
        "\n",
        "\n",
        "def DA_TimeWarp(X, sigma=0.2):\n",
        "    tt_new = DistortTimesteps(X, sigma)\n",
        "    X_new = np.zeros(X.shape)\n",
        "    x_range = np.arange(X.shape[0])\n",
        "    X_new[:, 0] = np.interp(x_range, tt_new[:, 0], X[:, 0])\n",
        "    X_new[:, 1] = np.interp(x_range, tt_new[:, 1], X[:, 1])\n",
        "    X_new[:, 2] = np.interp(x_range, tt_new[:, 2], X[:, 2])\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def time_warp(sample, choice, sigma=0.2):\n",
        "    if choice == 1:\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "        sample = DA_TimeWarp(sample, sigma=sigma)\n",
        "        sample = np.swapaxes(sample, 0, 1)\n",
        "    return sample\n",
        "\n",
        "def custom_virtual_data_generation_algorithm(data):\n",
        "\n",
        "     # Data augmentations\n",
        "    left = permute(data[:,:3].transpose(), 1)\n",
        "    right = time_warp(data[:,3:].transpose(), 1)\n",
        "\n",
        "    new_data = np.concatenate([left.transpose(), right.transpose()], axis=1)\n",
        "    return new_data\n",
        "\n",
        "def save_virtual_data(data, filename):\n",
        "\n",
        "  data.to_csv(os.path.join(virt_directory, filename+'.csv'), index=False)\n",
        "  return\n",
        "\n",
        "def custom_virtual_data_generation(train_data_dict):\n",
        "\n",
        "  for u, df in train_data_dict.items():\n",
        "    print('Generating virtual data from user %s.'% u)\n",
        "    # Extract sensor data and labels\n",
        "    raw_data = df[selected_columns[:6]].values\n",
        "    labels = df[selected_columns[-1]].values.reshape(-1,1)\n",
        "\n",
        "    tmp = custom_virtual_data_generation_algorithm(raw_data)\n",
        "\n",
        "    # Concatenate data with operation labels\n",
        "    virtual_data = np.concatenate([tmp, labels], axis=1)\n",
        "\n",
        "    # Convert np.array to dataframe\n",
        "    df = pd.DataFrame(virtual_data, columns=new_columns)\n",
        "\n",
        "    # Save data to /data/virtual/\n",
        "    save_virtual_data(df, u)\n",
        "    # df.to_csv(os.path.join(virt_directory, u+'.csv'), index=False)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create a new Python file and write the functions to it\n",
        "with open(rootdir+'/custom_functions.py', 'w') as file:\n",
        "    file.write(functions_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3PyqFMHNhhf0",
      "metadata": {
        "id": "3PyqFMHNhhf0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
